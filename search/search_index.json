{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Whole cell myocyte action potential analysis \u00a4 This repository has code to perform cardiac action potential analysis. It is primarily designed to analyze spontaneous cardiac action potentials from whole-cell current-clamp recordings of cardiac myocytes . SanPy, pronounced 'senpai' \u00a4 This is a work in progress, do not use this code. \u00a4 If you find the code in this repository interesting, please email Robert Cudmore at UC Davis ( rhcudmore@ucdavis.edu ) and we can get you started. We are looking for users and collaborators.","title":"Home"},{"location":"#whole-cell-myocyte-action-potential-analysis","text":"This repository has code to perform cardiac action potential analysis. It is primarily designed to analyze spontaneous cardiac action potentials from whole-cell current-clamp recordings of cardiac myocytes .","title":"Whole cell myocyte action potential analysis"},{"location":"#sanpy-pronounced-senpai","text":"","title":"SanPy, pronounced 'senpai'"},{"location":"#this-is-a-work-in-progress-do-not-use-this-code","text":"If you find the code in this repository interesting, please email Robert Cudmore at UC Davis ( rhcudmore@ucdavis.edu ) and we can get you started. We are looking for users and collaborators.","title":"This is a work in progress, do not use this code."},{"location":"about/","text":"If you find the code in this repository interesting, please email Robert Cudmore at UC Davis ( rhcudmore@ucdavis.edu ) and we can get you started. We are looking for users and collaborators. Please visit the Cudmore lab website for more information.","title":"About"},{"location":"api/","text":"Writing custom Python scripts \u00a4 In just a few lines of code, recordings can be loaded, analyzed, and plotted. See the /examples folder for examples. import matplotlib.pyplot as plt import bAnalysis import bAnalysisPlot ba = bAnalysis.bAnalysis('data/SAN-AP-example-Rs-change.abf') ba.spikeDetect() bAnalysisPlot.bPlot.plotSpikes(ba, xMin=140, xMax=145) plt.show()","title":"Overview"},{"location":"api/#writing-custom-python-scripts","text":"In just a few lines of code, recordings can be loaded, analyzed, and plotted. See the /examples folder for examples. import matplotlib.pyplot as plt import bAnalysis import bAnalysisPlot ba = bAnalysis.bAnalysis('data/SAN-AP-example-Rs-change.abf') ba.spikeDetect() bAnalysisPlot.bPlot.plotSpikes(ba, xMin=140, xMax=145) plt.show()","title":"Writing custom Python scripts"},{"location":"bAnalysis/","text":"bAnalysis \u00a4 The bAnalysis class wraps a pyabf file and adds spike detection, error checking, and saving of results. The underlying pyabf object is always available as self.abf . A bAnalysis object can be created with: An .abf file path. A .csv file with time/mV columns (todo: add this, for now use bAbfText.py). a byteStream when working in the cloud. a dictionary (todo: add this). Examples: ba = bAnalysis ( 'data/19114001.abf' ) print ( ba ) # prints info about underlying abf file ba . plotDeriv () ba . spikeDetect ( dvdtThreshold = 100 ) ba . plotSpikes () ba . plotClips () Link to class, first braket is name, second is link sanpy.bAnalysis Link to a member function sanpy.bAnalysis.bAnalysis.spikeDetect sanpy.bAnalysis.bAnalysis.makeSpikeClips Link to bExport and show link as bExport bExport This is an admonition with triple explamation points !!!. __init__ ( self , file = None , theTiff = None , byteStream = None ) special \u00a4 Initialize a bAnalysis object Parameters: Name Type Description Default file str Path to either .abf or .csv with time/mV columns. None theTiff str Path to .tif file. None byteStream binary Binary stream for use in the cloud. None Source code in sanpy/bAnalysis.py def __init__ ( self , file = None , theTiff = None , byteStream = None ): \"\"\"Initialize a bAnalysis object Args: file (str): Path to either .abf or .csv with time/mV columns. theTiff (str): Path to .tif file. byteStream (binary): Binary stream for use in the cloud. \"\"\" self . myFileType = None \"\"\"str: From ('abf', 'csv', 'tif', 'bytestream')\"\"\" self . loadError = False # abb 20201109 \"\"\"bool: True if error loading file/stream.\"\"\" self . detectionDict = None # remember the parameters of our last detection \"\"\"dict: Dictionary specifying detection parameters, see getDefaultDetection.\"\"\" self . file = file # todo: change this to filePath \"\"\"str: File path.\"\"\" self . _abf = None self . dateAnalyzed = None \"\"\"str: Date Time of analysis. TODO: make a property.\"\"\" self . detectionType = None \"\"\"str: From ('dvdt', 'mv')\"\"\" self . filteredVm = None self . filteredDeriv = None self . spikeDict = [] # a list of dict self . spikeTimes = [] # created in self.spikeDetect() self . spikeClips = [] # created in self.spikeDetect() if file is not None and not os . path . isfile ( file ): print ( f 'error: bAnalysis.__init__ file does not exist: { file } ' ) self . loadError = True return # only defined when loading abf files self . acqDate = None self . acqTime = None # instantiate and load abf file if byteStream is not None : #print(' bAnalysis() loading bytestream with pyabf.ABF()') self . _abf = pyabf . ABF ( byteStream ) self . myFileType = 'bytestream' elif file . endswith ( '.tif' ): self . _abf = sanpy . bAbfText ( file ) print ( ' === REMEMBER: bAnalysis.__init__() is normalizing Ca sweepY' ) self . _abf . sweepY = self . _normalizeData ( self . _abf . sweepY ) self . myFileType = 'tif' elif file . endswith ( '.csv' ): self . _abf = sanpy . bAbfText ( file ) self . myFileType = 'csv' elif file . endswith ( '.abf' ): try : self . _abf = pyabf . ABF ( file ) #20190621 abfDateTime = self . _abf . abfDateTime #2019-01-14 15:20:48.196000 self . acqDate = abfDateTime . strftime ( \"%Y-%m- %d \" ) self . acqTime = abfDateTime . strftime ( \"%H:%M:%S\" ) except ( NotImplementedError ) as e : print ( 'error: bAnalysis.__init__() did not load abf file:' , file ) print ( ' exception was:' , e ) self . loadError = True self . _abf = None return except ( Exception ) as e : # some abf files throw: 'unpack requires a buffer of 234 bytes' print ( 'error: bAnalysis.__init__() did not load abf file:' , file ) print ( ' unknown exception was:' , e ) self . loadError = True self . _abf = None return # we have a good abf file self . myFileType = 'abf' else : print ( f 'error: bAnalysis.__init__() can only open abf/csv/tif files: { file } ' ) self . loadError = True return # TODO: will not work for .tif # determine if current-clamp or voltage clamp self . _recordingMode = 'fix' self . _yUnits = 'fix' if self . _abf . sweepUnitsY in [ 'pA' ]: self . _recordingMode = 'V-Clamp' self . _yUnits = self . _abf . sweepUnitsY elif self . _abf . sweepUnitsY in [ 'mV' ]: self . _recordingMode = 'I-Clamp' self . _yUnits = self . _abf . sweepUnitsY self . currentSweep = None self . setSweep ( 0 ) self . filteredVm = [] self . filteredDeriv = [] self . spikeTimes = [] self . thresholdTimes = None # not used # get default derivative if self . _recordingMode == 'I-Clamp' : self . _getDerivative () elif self . _recordingMode == 'V-Clamp' : self . _getBaselineSubtract () else : self . _getDerivative () abf property readonly \u00a4 Get the underlying pyabf object. dataPointsPerMs property readonly \u00a4 Get 'data points per ms'. errorReport ( self ) \u00a4 Generate an error report, one row per error. Spikes can have more than one error. Returns: Type Description (pandas DataFrame) Pandas DataFrame, one row per error. Source code in sanpy/bAnalysis.py def errorReport ( self , ): \"\"\" Generate an error report, one row per error. Spikes can have more than one error. Returns: (pandas DataFrame): Pandas DataFrame, one row per error. \"\"\" dictList = [] numError = 0 errorList = [] for spikeIdx , spike in enumerate ( self . spikeDict ): for idx , error in enumerate ( spike [ 'errors' ]): # error is dict from _getErorDict if error is None or error == np . nan or error == 'nan' : continue dictList . append ( error ) if len ( dictList ) == 0 : fakeErrorDict = self . _getErrorDict ( 1 , 1 , 'fake' , 'fake' ) dfError = pd . DataFrame ( columns = fakeErrorDict . keys ()) else : dfError = pd . DataFrame ( dictList ) print ( 'bAnalysis.errorReport() returning len(dfError):' , len ( dfError )) return dfError getDefaultDetection ( self ) \u00a4 Get default detection dictionary, pass this to bAnalysis.spikeDetect() Returns: Type Description dict Dictionary of detection parameters. Source code in sanpy/bAnalysis.py def getDefaultDetection ( self ): \"\"\" Get default detection dictionary, pass this to [bAnalysis.spikeDetect()][sanpy.bAnalysis.bAnalysis.spikeDetect] Returns: dict: Dictionary of detection parameters. \"\"\" mvThreshold = - 20 theDict = { 'dvdtThreshold' : 100 , #if None then detect only using mvThreshold 'mvThreshold' : mvThreshold , 'medianFilter' : 0 , 'SavitzkyGolay_pnts' : 5 , # shoould correspond to about 0.5 ms 'SavitzkyGolay_poly' : 2 , 'halfHeights' : [ 10 , 20 , 50 , 80 , 90 ], # new 20210501 'mdp_ms' : 250 , # window before/after peak to look for MDP 'refractory_ms' : 170 , # rreject spikes with instantaneous frequency 'peakWindow_ms' : 100 , #10, # time after spike to look for AP peak 'dvdtPreWindow_ms' : 10 , #5, # used in dvdt, pre-roll to then search for real threshold crossing 'avgWindow_ms' : 5 , # 20210425, trying 0.15 #'dvdt_percentOfMax': 0.1, # only used in dvdt detection, used to back up spike threshold to more meaningful value 'dvdt_percentOfMax' : 0.1 , # only used in dvdt detection, used to back up spike threshold to more meaningful value # 20210413, was 50 for manuscript, we were missing lots of 1/2 widths 'halfWidthWindow_ms' : 200 , #200, #20, # add 20210413 to turn of doBackupSpikeVm on pure vm detection 'doBackupSpikeVm' : True , 'spikeClipWidth_ms' : 500 , 'onlyPeaksAbove_mV' : mvThreshold , 'startSeconds' : None , # not used ??? 'stopSeconds' : None , # for detection of Ca from line scans #'caThresholdPos': 0.01, #'caMinSpike': 0.5, # todo: get rid of this # book keeping like ('cellType', 'sex', 'condition') 'cellType' : '' , 'sex' : '' , 'condition' : '' , } return theDict . copy () getDefaultDetection_ca ( self ) \u00a4 Get default detection for Ca analysis. Warning, this is currently experimental. Returns: Type Description dict Dictionary of detection parameters. Source code in sanpy/bAnalysis.py def getDefaultDetection_ca ( self ): \"\"\" Get default detection for Ca analysis. Warning, this is currently experimental. Returns: dict: Dictionary of detection parameters. \"\"\" theDict = self . getDefaultDetection () theDict [ 'dvdtThreshold' ] = 0.01 #if None then detect only using mvThreshold theDict [ 'mvThreshold' ] = 0.5 # #theDict['medianFilter': 0 #'halfHeights': [20,50,80] theDict [ 'refractory_ms' ] = 200 #170 # reject spikes with instantaneous frequency #theDict['peakWindow_ms': 100 #10, # time after spike to look for AP peak #theDict['dvdtPreWindow_ms': 2 # used in dvdt, pre-roll to then search for real threshold crossing #theDict['avgWindow_ms': 5 #theDict['dvdt_percentOfMax': 0.1 theDict [ 'halfWidthWindow_ms' ] = 200 #was 20 #theDict['spikeClipWidth_ms': 500 #theDict['onlyPeaksAbove_mV': None #theDict['startSeconds': None #theDict['stopSeconds': None # for detection of Ca from line scans #theDict['caThresholdPos'] = 0.01 #theDict['caMinSpike'] = 0.5 return theDict . copy () getSpikeClips ( self , theMin , theMax ) \u00a4 get 2d list of spike clips, spike clips x, and 1d mean spike clip Parameters: Name Type Description Default theMin float Start seconds. required theMax float Stop seconds. required Source code in sanpy/bAnalysis.py def getSpikeClips ( self , theMin , theMax ): \"\"\" get 2d list of spike clips, spike clips x, and 1d mean spike clip Args: theMin (float): Start seconds. theMax (float): Stop seconds. \"\"\" if theMin is None or theMax is None : theMin = 0 theMax = self . abf . sweepX [ - 1 ] # make a list of clips within start/stop (Seconds) theseClips = [] theseClips_x = [] tmpClips = [] # for mean clip meanClip = [] #if start is not None and stop is not None: for idx , clip in enumerate ( self . spikeClips ): spikeTime = self . spikeTimes [ idx ] spikeTime = self . pnt2Sec_ ( spikeTime ) if spikeTime >= theMin and spikeTime <= theMax : theseClips . append ( clip ) theseClips_x . append ( self . spikeClips_x2 [ idx ]) # remember, all _x are the same if len ( self . spikeClips_x ) == len ( clip ): tmpClips . append ( clip ) # for mean clip if len ( tmpClips ): meanClip = np . mean ( tmpClips , axis = 0 ) return theseClips , theseClips_x , meanClip getStat ( self , statName1 , statName2 = None ) \u00a4 Get a list of values for one or two analysis parameters. For a list of available parameters use xxx. If the returned list of analysis parameters are in points, convert to seconds or ms using: pnt2Sec_(pnt) or pnt2Ms_(pnt). Parameters: Name Type Description Default statName1 str Name of the first analysis parameter to retreive. required statName2 str Optional, Name of the second analysis parameter to retreive. None Returns: Type Description list List of analysis parameter values, None if error. Source code in sanpy/bAnalysis.py def getStat ( self , statName1 , statName2 = None ): \"\"\" Get a list of values for one or two analysis parameters. For a list of available parameters use xxx. If the returned list of analysis parameters are in points, convert to seconds or ms using: pnt2Sec_(pnt) or pnt2Ms_(pnt). Args: statName1 (str): Name of the first analysis parameter to retreive. statName2 (str): Optional, Name of the second analysis parameter to retreive. Returns: list: List of analysis parameter values, None if error. \"\"\" def clean ( val ): if val is None : val = float ( 'nan' ) return val x = None y = None error = False if len ( self . spikeDict ) == 0 : #print('error bAnalysis.getStat(), there are no spikes') error = True elif not statName1 in self . spikeDict [ 0 ] . keys (): print ( 'error: bAnalysis.getStat() did not find statName1' , statName1 , 'in spikeDict' ) error = True elif statName2 is not None and not statName2 in self . spikeDict [ 0 ] . keys (): print ( 'error: bAnalysis.getStat() did not find statName2' , statName2 , 'in spikeDict' ) error = True if not error : x = [ clean ( spike [ statName1 ]) for spike in self . spikeDict ] if statName2 is not None : y = [ clean ( spike [ statName2 ]) for spike in self . spikeDict ] if statName2 is not None : return x , y else : return x getStatMean ( self , statName ) \u00a4 Get the mean of an analysis parameter. Parameters: Name Type Description Default statName str Name of the statistic to retreive. For a list of available stats use xxx. required Source code in sanpy/bAnalysis.py def getStatMean ( self , statName ): \"\"\" Get the mean of an analysis parameter. Args: statName (str): Name of the statistic to retreive. For a list of available stats use xxx. \"\"\" theMean = None x = self . getStat ( statName ) if x is not None and len ( x ) > 1 : theMean = np . nanmean ( x ) return theMean makeSpikeClips ( self , spikeClipWidth_ms , theseTime_sec = None ) \u00a4 Parameters: Name Type Description Default spikeClipWidth_ms int Width of each spike clip in milliseconds. required theseTime_sec list of float List of seconds to make clips from. None Returns: Type Description spikeClips_x2 ms spikeClips: sweepY Source code in sanpy/bAnalysis.py def makeSpikeClips ( self , spikeClipWidth_ms , theseTime_sec = None ): \"\"\" Args: spikeClipWidth_ms (int): Width of each spike clip in milliseconds. theseTime_sec (list of float): List of seconds to make clips from. Returns: spikeClips_x2: ms spikeClips: sweepY \"\"\" print ( 'makeSpikeClips() spikeClipWidth_ms:' , spikeClipWidth_ms , 'theseTime_sec:' , theseTime_sec ) if theseTime_sec is None : theseTime_pnts = self . spikeTimes else : # convert theseTime_sec to pnts theseTime_ms = [ x * 1000 for x in theseTime_sec ] theseTime_pnts = [ x * self . abf . dataPointsPerMs for x in theseTime_ms ] theseTime_pnts = [ round ( x ) for x in theseTime_pnts ] clipWidth_pnts = spikeClipWidth_ms * self . abf . dataPointsPerMs clipWidth_pnts = round ( clipWidth_pnts ) if clipWidth_pnts % 2 == 0 : pass # Even else : clipWidth_pnts += 1 # Make odd even halfClipWidth_pnts = int ( clipWidth_pnts / 2 ) print ( ' makeSpikeClips() clipWidth_pnts:' , clipWidth_pnts , 'halfClipWidth_pnts:' , halfClipWidth_pnts ) # make one x axis clip with the threshold crossing at 0 self . spikeClips_x = [( x - halfClipWidth_pnts ) / self . abf . dataPointsPerMs for x in range ( clipWidth_pnts )] #20190714, added this to make all clips same length, much easier to plot in MultiLine numPointsInClip = len ( self . spikeClips_x ) self . spikeClips = [] self . spikeClips_x2 = [] #for idx, spikeTime in enumerate(self.spikeTimes): for idx , spikeTime in enumerate ( theseTime_pnts ): #currentClip = vm[spikeTime-halfClipWidth_pnts:spikeTime+halfClipWidth_pnts] currentClip = self . abf . sweepY [ spikeTime - halfClipWidth_pnts : spikeTime + halfClipWidth_pnts ] if len ( currentClip ) == numPointsInClip : self . spikeClips . append ( currentClip ) self . spikeClips_x2 . append ( self . spikeClips_x ) # a 2D version to make pyqtgraph multiline happy else : pass ## print ( ' ERROR: bAnalysis.spikeDetect() did not add clip for spike index' , idx , 'at time' , spikeTime , 'currentClip:' , len ( currentClip ), 'numPointsInClip:' , numPointsInClip ) ## # return self . spikeClips_x2 , self . spikeClips ms2Pnt_ ( self , ms ) \u00a4 Convert milliseconds (ms) to point in recording using self.abf.dataPointsPerMs Parameters: Name Type Description Default ms float The ms into the recording required Returns: Type Description int The point in the recording Source code in sanpy/bAnalysis.py def ms2Pnt_ ( self , ms ): \"\"\" Convert milliseconds (ms) to point in recording using `self.abf.dataPointsPerMs` Args: ms (float): The ms into the recording Returns: int: The point in the recording \"\"\" theRet = ms * self . abf . dataPointsPerMs theRet = int ( theRet ) return theRet numSpikes property readonly \u00a4 Get the number of detected spikes numSweeps property readonly \u00a4 Get the number of sweeps. pnt2Ms_ ( self , pnt ) \u00a4 Convert a point to milliseconds (ms) using self.abf.dataPointsPerMs Parameters: Name Type Description Default pnt int The point required Returns: Type Description float The point in milliseconds (ms) Source code in sanpy/bAnalysis.py def pnt2Ms_ ( self , pnt ): \"\"\" Convert a point to milliseconds (ms) using `self.abf.dataPointsPerMs` Args: pnt (int): The point Returns: float: The point in milliseconds (ms) \"\"\" return pnt / self . abf . dataPointsPerMs pnt2Sec_ ( self , pnt ) \u00a4 Convert a point to Seconds using self.abf.dataPointsPerMs Parameters: Name Type Description Default pnt int The point required Returns: Type Description float The point in seconds Source code in sanpy/bAnalysis.py def pnt2Sec_ ( self , pnt ): \"\"\" Convert a point to Seconds using `self.abf.dataPointsPerMs` Args: pnt (int): The point Returns: float: The point in seconds \"\"\" if pnt is None : return math . isnan ( pnt ) else : return pnt / self . abf . dataPointsPerMs / 1000 setSweep ( self , sweepNumber ) \u00a4 Set the current sweep. Parameters: Name Type Description Default sweepNumber int The sweep number to set. required Source code in sanpy/bAnalysis.py def setSweep ( self , sweepNumber ): \"\"\" Set the current sweep. Args: sweepNumber (int): The sweep number to set. \"\"\" if sweepNumber not in self . abf . sweepList : print ( 'error: bAnalysis.setSweep() did not find sweep' , sweepNumber , ', sweepList =' , self . abf . sweepList ) return False else : self . currentSweep = sweepNumber self . abf . setSweep ( sweepNumber ) return True spikeDetect ( self , dDict , verbose = False ) \u00a4 Spike detect the current sweep and put results into self.spikeDict[] . Parameters: Name Type Description Default dDict dict A detection dictionary from bAnalysis.getDefaultDetection() required Source code in sanpy/bAnalysis.py def spikeDetect ( self , dDict , verbose = False ): ''' Spike detect the current sweep and put results into `self.spikeDict[]`. Args: dDict (dict): A detection dictionary from [bAnalysis.getDefaultDetection()][sanpy.bAnalysis.bAnalysis.getDefaultDetection] ''' self . detectionDict = dDict # remember the parameters of our last detection startTime = time . time () if verbose : sanpy . bUtil . printDict ( dDict ) if self . _recordingMode == 'I-Clamp' : self . _getDerivative ( dDict ) elif self . _recordingMode == 'V-Clamp' : self . _getBaselineSubtract ( dDict ) else : # in case recordingMode is ill defined self . _getDerivative ( dDict ) self . spikeDict = [] # we are filling this in, one dict for each spike detectionType = None # # spike detect if dDict [ 'dvdtThreshold' ] is None or np . isnan ( dDict [ 'dvdtThreshold' ]): # detect using mV threshold detectionType = 'mv' self . spikeTimes , spikeErrorList = self . _spikeDetect_vm ( dDict , verbose = verbose ) # backup childish vm threshold if dDict [ 'doBackupSpikeVm' ]: self . spikeTimes = self . _backupSpikeVm ( dDict [ 'medianFilter' ]) else : # detect using dv/dt threshold AND min mV detectionType = 'dvdt' self . spikeTimes , spikeErrorList = self . _spikeDetect_dvdt ( dDict , verbose = verbose ) vm = self . filteredVm dvdt = self . filteredDeriv # # look in a window after each threshold crossing to get AP peak peakWindow_pnts = self . abf . dataPointsPerMs * dDict [ 'peakWindow_ms' ] peakWindow_pnts = round ( peakWindow_pnts ) # # throw out spikes that have peak below onlyPeaksAbove_mV newSpikeTimes = [] newSpikeErrorList = [] if dDict [ 'onlyPeaksAbove_mV' ] is not None : for i , spikeTime in enumerate ( self . spikeTimes ): peakPnt = np . argmax ( vm [ spikeTime : spikeTime + peakWindow_pnts ]) peakPnt += spikeTime peakVal = np . max ( vm [ spikeTime : spikeTime + peakWindow_pnts ]) if peakVal > dDict [ 'onlyPeaksAbove_mV' ]: newSpikeTimes . append ( spikeTime ) newSpikeErrorList . append ( spikeErrorList [ i ]) else : print ( 'spikeDetect() peak height: rejecting spike' , i , 'at pnt:' , spikeTime , \"dDict['onlyPeaksAbove_mV']:\" , dDict [ 'onlyPeaksAbove_mV' ]) # self . spikeTimes = newSpikeTimes spikeErrorList = newSpikeErrorList # # throw out spikes on a down-slope avgWindow_pnts = dDict [ 'avgWindow_ms' ] * self . abf . dataPointsPerMs avgWindow_pnts = math . floor ( avgWindow_pnts / 2 ) for i , spikeTime in enumerate ( self . spikeTimes ): # spikeTime units is ALWAYS points peakPnt = np . argmax ( vm [ spikeTime : spikeTime + peakWindow_pnts ]) peakPnt += spikeTime peakVal = np . max ( vm [ spikeTime : spikeTime + peakWindow_pnts ]) spikeDict = OrderedDict () # use OrderedDict so Pandas output is in the correct order spikeDict [ 'include' ] = 1 spikeDict [ 'analysisVersion' ] = sanpy . analysisVersion spikeDict [ 'interfaceVersion' ] = sanpy . interfaceVersion spikeDict [ 'file' ] = self . file spikeDict [ 'detectionType' ] = detectionType spikeDict [ 'cellType' ] = dDict [ 'cellType' ] spikeDict [ 'sex' ] = dDict [ 'sex' ] spikeDict [ 'condition' ] = dDict [ 'condition' ] spikeDict [ 'spikeNumber' ] = i spikeDict [ 'errors' ] = [] # append existing spikeErrorList from spikeDetect_dvdt() or spikeDetect_mv() tmpError = spikeErrorList [ i ] if tmpError is not None and tmpError != np . nan : #spikeDict['numError'] += 1 spikeDict [ 'errors' ] . append ( tmpError ) # tmpError is from: #eDict = self._getErrorDict(i, spikeTime, errType, errStr) # spikeTime is in pnts # detection params spikeDict [ 'dvdtThreshold' ] = dDict [ 'dvdtThreshold' ] spikeDict [ 'mvThreshold' ] = dDict [ 'mvThreshold' ] spikeDict [ 'medianFilter' ] = dDict [ 'medianFilter' ] spikeDict [ 'halfHeights' ] = dDict [ 'halfHeights' ] spikeDict [ 'thresholdPnt' ] = spikeTime spikeDict [ 'thresholdVal' ] = vm [ spikeTime ] # in vm spikeDict [ 'thresholdVal_dvdt' ] = dvdt [ spikeTime ] # in dvdt spikeDict [ 'thresholdSec' ] = ( spikeTime / self . abf . dataPointsPerMs ) / 1000 spikeDict [ 'peakPnt' ] = peakPnt spikeDict [ 'peakVal' ] = peakVal spikeDict [ 'peakSec' ] = ( peakPnt / self . abf . dataPointsPerMs ) / 1000 spikeDict [ 'peakHeight' ] = spikeDict [ 'peakVal' ] - spikeDict [ 'thresholdVal' ] self . spikeDict . append ( spikeDict ) defaultVal = float ( 'nan' ) # get pre/post spike minima self . spikeDict [ i ][ 'preMinPnt' ] = None self . spikeDict [ i ][ 'preMinVal' ] = defaultVal #self.spikeDict[i]['postMinPnt'] = None #self.spikeDict[i]['postMinVal'] = defaultVal # early diastolic duration # 0.1 to 0.5 of time between pre spike min and spike time self . spikeDict [ i ][ 'preLinearFitPnt0' ] = None self . spikeDict [ i ][ 'preLinearFitPnt1' ] = None self . spikeDict [ i ][ 'earlyDiastolicDuration_ms' ] = defaultVal # seconds between preLinearFitPnt0 and preLinearFitPnt1 self . spikeDict [ i ][ 'preLinearFitVal0' ] = defaultVal self . spikeDict [ i ][ 'preLinearFitVal1' ] = defaultVal # m,b = np.polyfit(x, y, 1) self . spikeDict [ i ][ 'earlyDiastolicDurationRate' ] = defaultVal # fit of y=preLinearFitVal 0/1 versus x=preLinearFitPnt 0/1 self . spikeDict [ i ][ 'lateDiastolicDuration' ] = defaultVal # self . spikeDict [ i ][ 'preSpike_dvdt_max_pnt' ] = None self . spikeDict [ i ][ 'preSpike_dvdt_max_val' ] = defaultVal # in units mV self . spikeDict [ i ][ 'preSpike_dvdt_max_val2' ] = defaultVal # in units dv/dt self . spikeDict [ i ][ 'postSpike_dvdt_min_pnt' ] = None self . spikeDict [ i ][ 'postSpike_dvdt_min_val' ] = defaultVal # in units mV self . spikeDict [ i ][ 'postSpike_dvdt_min_val2' ] = defaultVal # in units dv/dt self . spikeDict [ i ][ 'isi_pnts' ] = defaultVal # time between successive AP thresholds (thresholdSec) self . spikeDict [ i ][ 'isi_ms' ] = defaultVal # time between successive AP thresholds (thresholdSec) self . spikeDict [ i ][ 'spikeFreq_hz' ] = defaultVal # time between successive AP thresholds (thresholdSec) self . spikeDict [ i ][ 'cycleLength_pnts' ] = defaultVal # time between successive MDPs self . spikeDict [ i ][ 'cycleLength_ms' ] = defaultVal # time between successive MDPs # Action potential duration (APD) was defined as the interval between the TOP and the subsequent MDP #self.spikeDict[i]['apDuration_ms'] = defaultVal self . spikeDict [ i ][ 'diastolicDuration_ms' ] = defaultVal # any number of spike widths self . spikeDict [ i ][ 'widths' ] = [] for halfHeight in dDict [ 'halfHeights' ]: widthDict = { 'halfHeight' : halfHeight , 'risingPnt' : None , 'risingVal' : defaultVal , 'fallingPnt' : None , 'fallingVal' : defaultVal , 'widthPnts' : None , 'widthMs' : defaultVal } # abb 20210125, make column width_<n> where <n> is 'halfHeight' self . spikeDict [ i ][ 'widths_' + str ( halfHeight )] = defaultVal # was this self . spikeDict [ i ][ 'widths' ] . append ( widthDict ) # The nonlinear late diastolic depolarization phase was estimated as the duration between 1% and 10% dV/dt # todo: not done !!!!!!!!!! # 20210413, was this. This next block is for pre spike analysis # ok if we are at last spike #if i==0 or i==len(self.spikeTimes)-1: if i == 0 : # was continue but moved half width out of here pass else : mdp_ms = dDict [ 'mdp_ms' ] mdp_pnts = mdp_ms * self . abf . dataPointsPerMs #print('bAnalysis.spikeDetect() needs to be int mdp_pnts:', mdp_pnts) mdp_pnts = int ( mdp_pnts ) # # pre spike min #preRange = vm[self.spikeTimes[i-1]:self.spikeTimes[i]] startPnt = self . spikeTimes [ i ] - mdp_pnts #print(' xxx preRange:', i, startPnt, self.spikeTimes[i]) if startPnt < 0 : # for V-Clammp startPnt = 0 preRange = vm [ startPnt : self . spikeTimes [ i ]] # EXCEPTION preMinPnt = np . argmin ( preRange ) #preMinPnt += self.spikeTimes[i-1] preMinPnt += startPnt # the pre min is actually an average around the real minima avgRange = vm [ preMinPnt - avgWindow_pnts : preMinPnt + avgWindow_pnts ] preMinVal = np . average ( avgRange ) # search backward from spike to find when vm reaches preMinVal (avg) preRange = vm [ preMinPnt : self . spikeTimes [ i ]] preRange = np . flip ( preRange ) # we want to search backwards from peak try : preMinPnt2 = np . where ( preRange < preMinVal )[ 0 ][ 0 ] preMinPnt = self . spikeTimes [ i ] - preMinPnt2 self . spikeDict [ i ][ 'preMinPnt' ] = preMinPnt self . spikeDict [ i ][ 'preMinVal' ] = preMinVal except ( IndexError ) as e : #self.spikeDict[i]['numError'] = self.spikeDict[i]['numError'] + 1 # sometimes preRange is empty, don't try and put min/max in error #print('heroku preMinValue error !!!!!!!!!!!!!!!') #print(' ', self.spikeDict[i]) errorStr = 'searching for preMinVal:' + str ( preMinVal ) #+ ' postRange min:' + str(np.min(postRange)) + ' max ' + str(np.max(postRange)) eDict = self . _getErrorDict ( i , self . spikeTimes [ i ], 'preMin' , errorStr ) # spikeTime is in pnts self . spikeDict [ i ][ 'errors' ] . append ( eDict ) # # linear fit on 10% - 50% of the time from preMinPnt to self.spikeTimes[i] startLinearFit = 0.1 # percent of time between pre spike min and AP peak stopLinearFit = 0.5 # percent of time between pre spike min and AP peak # taking floor() so we always get an integer # points timeInterval_pnts = math . floor ( self . spikeTimes [ i ] - preMinPnt ) preLinearFitPnt0 = preMinPnt + math . floor ( timeInterval_pnts * startLinearFit ) preLinearFitPnt1 = preMinPnt + math . floor ( timeInterval_pnts * stopLinearFit ) preLinearFitVal0 = vm [ preLinearFitPnt0 ] preLinearFitVal1 = vm [ preLinearFitPnt1 ] # linear fit before spike self . spikeDict [ i ][ 'preLinearFitPnt0' ] = preLinearFitPnt0 self . spikeDict [ i ][ 'preLinearFitPnt1' ] = preLinearFitPnt1 self . spikeDict [ i ][ 'earlyDiastolicDuration_ms' ] = self . pnt2Ms_ ( preLinearFitPnt1 - preLinearFitPnt0 ) self . spikeDict [ i ][ 'preLinearFitVal0' ] = preLinearFitVal0 self . spikeDict [ i ][ 'preLinearFitVal1' ] = preLinearFitVal1 # a linear fit where 'm,b = np.polyfit(x, y, 1)' # m*x+b\" xFit = self . abf . sweepX [ preLinearFitPnt0 : preLinearFitPnt1 ] yFit = vm [ preLinearFitPnt0 : preLinearFitPnt1 ] with warnings . catch_warnings (): warnings . filterwarnings ( 'error' ) try : mLinear , bLinear = np . polyfit ( xFit , yFit , 1 ) # m is slope, b is intercept self . spikeDict [ i ][ 'earlyDiastolicDurationRate' ] = mLinear except TypeError : #catching exception: raise TypeError(\"expected non-empty vector for x\") self . spikeDict [ i ][ 'earlyDiastolicDurationRate' ] = defaultVal errorStr = 'earlyDiastolicDurationRate fit' eDict = self . _getErrorDict ( i , self . spikeTimes [ i ], 'fitEDD' , errorStr ) # spikeTime is in pnts self . spikeDict [ i ][ 'errors' ] . append ( eDict ) except np . RankWarning : # also throws: RankWarning: Polyfit may be poorly conditioned self . spikeDict [ i ][ 'earlyDiastolicDurationRate' ] = defaultVal errorStr = 'earlyDiastolicDurationRate fit - RankWarning' eDict = self . _getErrorDict ( i , self . spikeTimes [ i ], 'fitEDD2' , errorStr ) # spikeTime is in pnts self . spikeDict [ i ][ 'errors' ] . append ( eDict ) # not implemented #self.spikeDict[i]['lateDiastolicDuration'] = ??? if True : # # maxima in dv/dt before spike # added try/except sunday april 14, seems to break spike detection??? try : # 20210415 was this #preRange = dvdt[self.spikeTimes[i]:peakPnt] preRange = dvdt [ self . spikeTimes [ i ]: peakPnt + 1 ] preSpike_dvdt_max_pnt = np . argmax ( preRange ) preSpike_dvdt_max_pnt += self . spikeTimes [ i ] self . spikeDict [ i ][ 'preSpike_dvdt_max_pnt' ] = preSpike_dvdt_max_pnt self . spikeDict [ i ][ 'preSpike_dvdt_max_val' ] = vm [ preSpike_dvdt_max_pnt ] # in units mV self . spikeDict [ i ][ 'preSpike_dvdt_max_val2' ] = dvdt [ preSpike_dvdt_max_pnt ] # in units mV except ( ValueError ) as e : #self.spikeDict[i]['numError'] = self.spikeDict[i]['numError'] + 1 # sometimes preRange is empty, don't try and put min/max in error #print('preRange:', preRange) errorStr = 'searching for preSpike_dvdt_max_pnt:' eDict = self . _getErrorDict ( i , self . spikeTimes [ i ], 'preSpikeDvDt' , errorStr ) # spikeTime is in pnts self . spikeDict [ i ][ 'errors' ] . append ( eDict ) # 20210501, we do not need postMin/mdp, not used anywhere else ''' if i==len(self.spikeTimes)-1: # last spike pass else: # # post spike min postRange = vm[self.spikeTimes[i]:self.spikeTimes[i+1]] postMinPnt = np.argmin(postRange) postMinPnt += self.spikeTimes[i] # the post min is actually an average around the real minima avgRange = vm[postMinPnt-avgWindow_pnts:postMinPnt+avgWindow_pnts] postMinVal = np.average(avgRange) # search forward from spike to find when vm reaches postMinVal (avg) postRange = vm[self.spikeTimes[i]:postMinPnt] try: postMinPnt2 = np.where(postRange<postMinVal)[0][0] postMinPnt = self.spikeTimes[i] + postMinPnt2 self.spikeDict[i]['postMinPnt'] = postMinPnt self.spikeDict[i]['postMinVal'] = postMinVal except (IndexError) as e: self.spikeDict[i]['numError'] = self.spikeDict[i]['numError'] + 1 # sometimes postRange is empty, don't try and put min/max in error #print('postRange:', postRange) errorStr = 'searching for postMinVal:' + str(postMinVal) #+ ' postRange min:' + str(np.min(postRange)) + ' max ' + str(np.max(postRange)) eDict = self._getErrorDict(i, self.spikeTimes[i], 'postMinError', errorStr) # spikeTime is in pnts self.spikeDict[i]['errors'].append(eDict) ''' if True : # # minima in dv/dt after spike #postRange = dvdt[self.spikeTimes[i]:postMinPnt] postSpike_ms = 10 postSpike_pnts = self . abf . dataPointsPerMs * postSpike_ms # abb 20210130 lcr analysis postSpike_pnts = round ( postSpike_pnts ) #postRange = dvdt[self.spikeTimes[i]:self.spikeTimes[i]+postSpike_pnts] # fixed window after spike postRange = dvdt [ peakPnt : peakPnt + postSpike_pnts ] # fixed window after spike postSpike_dvdt_min_pnt = np . argmin ( postRange ) postSpike_dvdt_min_pnt += peakPnt self . spikeDict [ i ][ 'postSpike_dvdt_min_pnt' ] = postSpike_dvdt_min_pnt self . spikeDict [ i ][ 'postSpike_dvdt_min_val' ] = vm [ postSpike_dvdt_min_pnt ] self . spikeDict [ i ][ 'postSpike_dvdt_min_val2' ] = dvdt [ postSpike_dvdt_min_pnt ] # 202102 #self.spikeDict[i]['preMinPnt'] = preMinPnt #self.spikeDict[i]['preMinVal'] = preMinVal # 202102 #self.spikeDict[i]['postMinPnt'] = postMinPnt #self.spikeDict[i]['postMinVal'] = postMinVal # linear fit before spike #self.spikeDict[i]['preLinearFitPnt0'] = preLinearFitPnt0 #self.spikeDict[i]['preLinearFitPnt1'] = preLinearFitPnt1 #self.spikeDict[i]['earlyDiastolicDuration_ms'] = self.pnt2Ms_(preLinearFitPnt1 - preLinearFitPnt0) #self.spikeDict[i]['preLinearFitVal0'] = preLinearFitVal0 #self.spikeDict[i]['preLinearFitVal1'] = preLinearFitVal1 # # Action potential duration (APD) was defined as # the interval between the TOP and the subsequent MDP # 20210501, removed AP duration, use APD_90, APD_50 etc ''' if i==len(self.spikeTimes)-1: pass else: self.spikeDict[i]['apDuration_ms'] = self.pnt2Ms_(postMinPnt - spikeDict['thresholdPnt']) ''' # # diastolic duration was defined as # the interval between MDP and TOP if i > 0 : # one off error when preMinPnt is not defined self . spikeDict [ i ][ 'diastolicDuration_ms' ] = self . pnt2Ms_ ( spikeTime - preMinPnt ) self . spikeDict [ i ][ 'cycleLength_ms' ] = float ( 'nan' ) if i > 0 : #20190627, was i>1 isiPnts = self . spikeDict [ i ][ 'thresholdPnt' ] - self . spikeDict [ i - 1 ][ 'thresholdPnt' ] isi_ms = self . pnt2Ms_ ( isiPnts ) isi_hz = 1 / ( isi_ms / 1000 ) self . spikeDict [ i ][ 'isi_pnts' ] = isiPnts self . spikeDict [ i ][ 'isi_ms' ] = self . pnt2Ms_ ( isiPnts ) self . spikeDict [ i ][ 'spikeFreq_hz' ] = 1 / ( self . pnt2Ms_ ( isiPnts ) / 1000 ) # Cycle length was defined as the interval between MDPs in successive APs prevPreMinPnt = self . spikeDict [ i - 1 ][ 'preMinPnt' ] # can be nan thisPreMinPnt = self . spikeDict [ i ][ 'preMinPnt' ] if prevPreMinPnt is not None and thisPreMinPnt is not None : cycleLength_pnts = thisPreMinPnt - prevPreMinPnt self . spikeDict [ i ][ 'cycleLength_pnts' ] = cycleLength_pnts self . spikeDict [ i ][ 'cycleLength_ms' ] = self . pnt2Ms_ ( cycleLength_pnts ) else : # error #self.spikeDict[i]['numError'] = self.spikeDict[i]['numError'] + 1 errorStr = 'previous spike preMinPnt is ' + str ( prevPreMinPnt ) + ' this preMinPnt:' + str ( thisPreMinPnt ) eDict = self . _getErrorDict ( i , self . spikeTimes [ i ], 'cycleLength' , errorStr ) # spikeTime is in pnts self . spikeDict [ i ][ 'errors' ] . append ( eDict ) ''' # 20210501 was this, I am no longer using postMinPnt prevPostMinPnt = self.spikeDict[i-1]['postMinPnt'] tmpPostMinPnt = self.spikeDict[i]['postMinPnt'] if prevPostMinPnt is not None and tmpPostMinPnt is not None: cycleLength_pnts = tmpPostMinPnt - prevPostMinPnt self.spikeDict[i]['cycleLength_pnts'] = cycleLength_pnts self.spikeDict[i]['cycleLength_ms'] = self.pnt2Ms_(cycleLength_pnts) else: self.spikeDict[i]['numError'] = self.spikeDict[i]['numError'] + 1 errorStr = 'previous spike postMinPnt is ' + str(prevPostMinPnt) + ' this postMinPnt:' + str(tmpPostMinPnt) eDict = self._getErrorDict(i, self.spikeTimes[i], 'cycleLength', errorStr) # spikeTime is in pnts self.spikeDict[i]['errors'].append(eDict) ''' # # spike half with and APDur # # # 20210130, moving 'half width' out of inner if spike # is first/last # # get 1/2 height (actually, any number of height measurements) # action potential duration using peak and post min #self.spikeDict[i]['widths'] = [] #print('*** calculating half width for spike', i) doWidthVersion2 = False #halfWidthWindow_ms = 20 hwWindowPnts = dDict [ 'halfWidthWindow_ms' ] * self . abf . dataPointsPerMs hwWindowPnts = round ( hwWindowPnts ) tmpPeakSec = spikeDict [ 'peakSec' ] tmpErrorType = None for j , halfHeight in enumerate ( dDict [ 'halfHeights' ]): # halfHeight in [20, 50, 80] if doWidthVersion2 : tmpThreshVm = spikeDict [ 'thresholdVal' ] thisVm = tmpThreshVm + ( peakVal - tmpThreshVm ) * ( halfHeight * 0.01 ) else : # 20210413 was this #thisVm = postMinVal + (peakVal - postMinVal) * (halfHeight * 0.01) tmpThreshVm2 = spikeDict [ 'thresholdVal' ] thisVm = tmpThreshVm2 + ( peakVal - tmpThreshVm2 ) * ( halfHeight * 0.01 ) #todo: logic is broken, this get over-written in following try widthDict = { 'halfHeight' : halfHeight , 'risingPnt' : None , 'risingVal' : defaultVal , 'fallingPnt' : None , 'fallingVal' : defaultVal , 'widthPnts' : None , 'widthMs' : defaultVal } widthMs = np . nan try : if doWidthVersion2 : postRange = vm [ peakPnt : peakPnt + hwWindowPnts ] else : # 20210413 was this #postRange = vm[peakPnt:postMinPnt] postRange = vm [ peakPnt : peakPnt + hwWindowPnts ] fallingPnt = np . where ( postRange < thisVm )[ 0 ] # less than if len ( fallingPnt ) == 0 : #error tmpErrorType = 'falling point' raise IndexError fallingPnt = fallingPnt [ 0 ] # first falling point fallingPnt += peakPnt fallingVal = vm [ fallingPnt ] # use the post/falling to find pre/rising if doWidthVersion2 : preRange = vm [ peakPnt - hwWindowPnts : peakPnt ] else : tmpPreMinPnt2 = spikeDict [ 'thresholdPnt' ] preRange = vm [ tmpPreMinPnt2 : peakPnt ] risingPnt = np . where ( preRange > fallingVal )[ 0 ] # greater than if len ( risingPnt ) == 0 : #error tmpErrorType = 'rising point' raise IndexError risingPnt = risingPnt [ 0 ] # first falling point if doWidthVersion2 : risingPnt += peakPnt - hwWindowPnts else : risingPnt += spikeDict [ 'thresholdPnt' ] risingVal = vm [ risingPnt ] # width (pnts) widthPnts = fallingPnt - risingPnt # 20210413 widthPnts2 = fallingPnt - spikeDict [ 'thresholdPnt' ] tmpRisingPnt = spikeDict [ 'thresholdPnt' ] # assign widthDict [ 'halfHeight' ] = halfHeight # 20210413, put back in #widthDict['risingPnt'] = risingPnt widthDict [ 'risingPnt' ] = tmpRisingPnt widthDict [ 'risingVal' ] = risingVal widthDict [ 'fallingPnt' ] = fallingPnt widthDict [ 'fallingVal' ] = fallingVal widthDict [ 'widthPnts' ] = widthPnts widthDict [ 'widthMs' ] = widthPnts / self . abf . dataPointsPerMs widthMs = widthPnts / self . abf . dataPointsPerMs # abb 20210125 # 20210413, todo: make these end in 2 widthDict [ 'widthPnts' ] = widthPnts2 widthDict [ 'widthMs' ] = widthPnts / self . abf . dataPointsPerMs except ( IndexError ) as e : ## ## #print(' EXCEPTION: bAnalysis.spikeDetect() spike', i, 'half height', halfHeight) ## ## #self.spikeDict[i]['numError'] = self.spikeDict[i]['numError'] + 1 #errorStr = 'spike ' + str(i) + ' half width ' + str(tmpErrorType) + ' ' + str(halfHeight) + ' halfWidthWindow_ms:' + str(dDict['halfWidthWindow_ms']) errorStr = ( f 'half width { halfHeight } error in { tmpErrorType } ' f \"with halfWidthWindow_ms: { dDict [ 'halfWidthWindow_ms' ] } \" f 'searching for Vm: { round ( thisVm , 2 ) } from peak sec { round ( tmpPeakSec , 2 ) } ' ) eDict = self . _getErrorDict ( i , self . spikeTimes [ i ], 'spikeWidth' , errorStr ) # spikeTime is in pnts self . spikeDict [ i ][ 'errors' ] . append ( eDict ) # abb 20210125 # wtf is hapenning on Heroku???? #print('**** heroku debug i:', i, 'j:', j, 'len:', len(self.spikeDict), 'halfHeight:', halfHeight) #print(' self.spikeDict[i]', self.spikeDict[i]['widths_'+str(halfHeight)]) self . spikeDict [ i ][ 'widths_' + str ( halfHeight )] = widthMs self . spikeDict [ i ][ 'widths' ][ j ] = widthDict # # look between threshold crossing to get minima # we will ignore the first and last spike # todo: call self.makeSpikeClips() # # build a list of spike clips #clipWidth_ms = 500 clipWidth_pnts = dDict [ 'spikeClipWidth_ms' ] * self . abf . dataPointsPerMs clipWidth_pnts = round ( clipWidth_pnts ) if clipWidth_pnts % 2 == 0 : pass # Even else : clipWidth_pnts += 1 # Odd halfClipWidth_pnts = int ( clipWidth_pnts / 2 ) print ( ' spikeDetect() clipWidth_pnts:' , clipWidth_pnts , 'halfClipWidth_pnts:' , halfClipWidth_pnts ) # make one x axis clip with the threshold crossing at 0 self . spikeClips_x = [( x - halfClipWidth_pnts ) / self . abf . dataPointsPerMs for x in range ( clipWidth_pnts )] #20190714, added this to make all clips same length, much easier to plot in MultiLine numPointsInClip = len ( self . spikeClips_x ) self . spikeClips = [] self . spikeClips_x2 = [] for idx , spikeTime in enumerate ( self . spikeTimes ): #currentClip = vm[spikeTime-halfClipWidth_pnts:spikeTime+halfClipWidth_pnts] currentClip = vm [ spikeTime - halfClipWidth_pnts : spikeTime + halfClipWidth_pnts ] if len ( currentClip ) == numPointsInClip : self . spikeClips . append ( currentClip ) self . spikeClips_x2 . append ( self . spikeClips_x ) # a 2D version to make pyqtgraph multiline happy else : ## ## if idx == 0 or idx == len ( self . spikeTimes ) - 1 : # don't report spike clip errors for first/last spike pass else : print ( ' ERROR: bAnalysis.spikeDetect() did not add clip for spike index' , idx , 'at time' , spikeTime , 'currentClip:' , len ( currentClip ), 'numPointsInClip:' , numPointsInClip ) ## ## # 20210426 # generate a df holding stats (used by scatterplotwidget) print ( ' making df to pass to scatterplotwidget' ) startSeconds = dDict [ 'startSeconds' ] stopSeconds = dDict [ 'stopSeconds' ] #tmpAnalysisName, df0 = self.getReportDf(theMin, theMax, savefile) exportObject = sanpy . bExport ( self ) dfReportForScatter = exportObject . report ( startSeconds , stopSeconds ) stopTime = time . time () if verbose : print ( 'bAnalysis.spikeDetect() for file' , self . file ) print ( ' detected' , len ( self . spikeTimes ), 'spikes in' , round ( stopTime - startTime , 2 ), 'seconds' ) self . errorReport () return dfReportForScatter sweepList property readonly \u00a4 Get a list of sweeps.","title":"bAnalysis"},{"location":"bAnalysis/#sanpy.bAnalysis.bAnalysis","text":"The bAnalysis class wraps a pyabf file and adds spike detection, error checking, and saving of results. The underlying pyabf object is always available as self.abf . A bAnalysis object can be created with: An .abf file path. A .csv file with time/mV columns (todo: add this, for now use bAbfText.py). a byteStream when working in the cloud. a dictionary (todo: add this). Examples: ba = bAnalysis ( 'data/19114001.abf' ) print ( ba ) # prints info about underlying abf file ba . plotDeriv () ba . spikeDetect ( dvdtThreshold = 100 ) ba . plotSpikes () ba . plotClips () Link to class, first braket is name, second is link sanpy.bAnalysis Link to a member function sanpy.bAnalysis.bAnalysis.spikeDetect sanpy.bAnalysis.bAnalysis.makeSpikeClips Link to bExport and show link as bExport bExport This is an admonition with triple explamation points !!!.","title":"bAnalysis"},{"location":"bAnalysis/#sanpy.bAnalysis.bAnalysis.__init__","text":"Initialize a bAnalysis object Parameters: Name Type Description Default file str Path to either .abf or .csv with time/mV columns. None theTiff str Path to .tif file. None byteStream binary Binary stream for use in the cloud. None Source code in sanpy/bAnalysis.py def __init__ ( self , file = None , theTiff = None , byteStream = None ): \"\"\"Initialize a bAnalysis object Args: file (str): Path to either .abf or .csv with time/mV columns. theTiff (str): Path to .tif file. byteStream (binary): Binary stream for use in the cloud. \"\"\" self . myFileType = None \"\"\"str: From ('abf', 'csv', 'tif', 'bytestream')\"\"\" self . loadError = False # abb 20201109 \"\"\"bool: True if error loading file/stream.\"\"\" self . detectionDict = None # remember the parameters of our last detection \"\"\"dict: Dictionary specifying detection parameters, see getDefaultDetection.\"\"\" self . file = file # todo: change this to filePath \"\"\"str: File path.\"\"\" self . _abf = None self . dateAnalyzed = None \"\"\"str: Date Time of analysis. TODO: make a property.\"\"\" self . detectionType = None \"\"\"str: From ('dvdt', 'mv')\"\"\" self . filteredVm = None self . filteredDeriv = None self . spikeDict = [] # a list of dict self . spikeTimes = [] # created in self.spikeDetect() self . spikeClips = [] # created in self.spikeDetect() if file is not None and not os . path . isfile ( file ): print ( f 'error: bAnalysis.__init__ file does not exist: { file } ' ) self . loadError = True return # only defined when loading abf files self . acqDate = None self . acqTime = None # instantiate and load abf file if byteStream is not None : #print(' bAnalysis() loading bytestream with pyabf.ABF()') self . _abf = pyabf . ABF ( byteStream ) self . myFileType = 'bytestream' elif file . endswith ( '.tif' ): self . _abf = sanpy . bAbfText ( file ) print ( ' === REMEMBER: bAnalysis.__init__() is normalizing Ca sweepY' ) self . _abf . sweepY = self . _normalizeData ( self . _abf . sweepY ) self . myFileType = 'tif' elif file . endswith ( '.csv' ): self . _abf = sanpy . bAbfText ( file ) self . myFileType = 'csv' elif file . endswith ( '.abf' ): try : self . _abf = pyabf . ABF ( file ) #20190621 abfDateTime = self . _abf . abfDateTime #2019-01-14 15:20:48.196000 self . acqDate = abfDateTime . strftime ( \"%Y-%m- %d \" ) self . acqTime = abfDateTime . strftime ( \"%H:%M:%S\" ) except ( NotImplementedError ) as e : print ( 'error: bAnalysis.__init__() did not load abf file:' , file ) print ( ' exception was:' , e ) self . loadError = True self . _abf = None return except ( Exception ) as e : # some abf files throw: 'unpack requires a buffer of 234 bytes' print ( 'error: bAnalysis.__init__() did not load abf file:' , file ) print ( ' unknown exception was:' , e ) self . loadError = True self . _abf = None return # we have a good abf file self . myFileType = 'abf' else : print ( f 'error: bAnalysis.__init__() can only open abf/csv/tif files: { file } ' ) self . loadError = True return # TODO: will not work for .tif # determine if current-clamp or voltage clamp self . _recordingMode = 'fix' self . _yUnits = 'fix' if self . _abf . sweepUnitsY in [ 'pA' ]: self . _recordingMode = 'V-Clamp' self . _yUnits = self . _abf . sweepUnitsY elif self . _abf . sweepUnitsY in [ 'mV' ]: self . _recordingMode = 'I-Clamp' self . _yUnits = self . _abf . sweepUnitsY self . currentSweep = None self . setSweep ( 0 ) self . filteredVm = [] self . filteredDeriv = [] self . spikeTimes = [] self . thresholdTimes = None # not used # get default derivative if self . _recordingMode == 'I-Clamp' : self . _getDerivative () elif self . _recordingMode == 'V-Clamp' : self . _getBaselineSubtract () else : self . _getDerivative ()","title":"__init__()"},{"location":"bAnalysis/#sanpy.bAnalysis.bAnalysis.abf","text":"Get the underlying pyabf object.","title":"abf"},{"location":"bAnalysis/#sanpy.bAnalysis.bAnalysis.dataPointsPerMs","text":"Get 'data points per ms'.","title":"dataPointsPerMs"},{"location":"bAnalysis/#sanpy.bAnalysis.bAnalysis.errorReport","text":"Generate an error report, one row per error. Spikes can have more than one error. Returns: Type Description (pandas DataFrame) Pandas DataFrame, one row per error. Source code in sanpy/bAnalysis.py def errorReport ( self , ): \"\"\" Generate an error report, one row per error. Spikes can have more than one error. Returns: (pandas DataFrame): Pandas DataFrame, one row per error. \"\"\" dictList = [] numError = 0 errorList = [] for spikeIdx , spike in enumerate ( self . spikeDict ): for idx , error in enumerate ( spike [ 'errors' ]): # error is dict from _getErorDict if error is None or error == np . nan or error == 'nan' : continue dictList . append ( error ) if len ( dictList ) == 0 : fakeErrorDict = self . _getErrorDict ( 1 , 1 , 'fake' , 'fake' ) dfError = pd . DataFrame ( columns = fakeErrorDict . keys ()) else : dfError = pd . DataFrame ( dictList ) print ( 'bAnalysis.errorReport() returning len(dfError):' , len ( dfError )) return dfError","title":"errorReport()"},{"location":"bAnalysis/#sanpy.bAnalysis.bAnalysis.getDefaultDetection","text":"Get default detection dictionary, pass this to bAnalysis.spikeDetect() Returns: Type Description dict Dictionary of detection parameters. Source code in sanpy/bAnalysis.py def getDefaultDetection ( self ): \"\"\" Get default detection dictionary, pass this to [bAnalysis.spikeDetect()][sanpy.bAnalysis.bAnalysis.spikeDetect] Returns: dict: Dictionary of detection parameters. \"\"\" mvThreshold = - 20 theDict = { 'dvdtThreshold' : 100 , #if None then detect only using mvThreshold 'mvThreshold' : mvThreshold , 'medianFilter' : 0 , 'SavitzkyGolay_pnts' : 5 , # shoould correspond to about 0.5 ms 'SavitzkyGolay_poly' : 2 , 'halfHeights' : [ 10 , 20 , 50 , 80 , 90 ], # new 20210501 'mdp_ms' : 250 , # window before/after peak to look for MDP 'refractory_ms' : 170 , # rreject spikes with instantaneous frequency 'peakWindow_ms' : 100 , #10, # time after spike to look for AP peak 'dvdtPreWindow_ms' : 10 , #5, # used in dvdt, pre-roll to then search for real threshold crossing 'avgWindow_ms' : 5 , # 20210425, trying 0.15 #'dvdt_percentOfMax': 0.1, # only used in dvdt detection, used to back up spike threshold to more meaningful value 'dvdt_percentOfMax' : 0.1 , # only used in dvdt detection, used to back up spike threshold to more meaningful value # 20210413, was 50 for manuscript, we were missing lots of 1/2 widths 'halfWidthWindow_ms' : 200 , #200, #20, # add 20210413 to turn of doBackupSpikeVm on pure vm detection 'doBackupSpikeVm' : True , 'spikeClipWidth_ms' : 500 , 'onlyPeaksAbove_mV' : mvThreshold , 'startSeconds' : None , # not used ??? 'stopSeconds' : None , # for detection of Ca from line scans #'caThresholdPos': 0.01, #'caMinSpike': 0.5, # todo: get rid of this # book keeping like ('cellType', 'sex', 'condition') 'cellType' : '' , 'sex' : '' , 'condition' : '' , } return theDict . copy ()","title":"getDefaultDetection()"},{"location":"bAnalysis/#sanpy.bAnalysis.bAnalysis.getDefaultDetection_ca","text":"Get default detection for Ca analysis. Warning, this is currently experimental. Returns: Type Description dict Dictionary of detection parameters. Source code in sanpy/bAnalysis.py def getDefaultDetection_ca ( self ): \"\"\" Get default detection for Ca analysis. Warning, this is currently experimental. Returns: dict: Dictionary of detection parameters. \"\"\" theDict = self . getDefaultDetection () theDict [ 'dvdtThreshold' ] = 0.01 #if None then detect only using mvThreshold theDict [ 'mvThreshold' ] = 0.5 # #theDict['medianFilter': 0 #'halfHeights': [20,50,80] theDict [ 'refractory_ms' ] = 200 #170 # reject spikes with instantaneous frequency #theDict['peakWindow_ms': 100 #10, # time after spike to look for AP peak #theDict['dvdtPreWindow_ms': 2 # used in dvdt, pre-roll to then search for real threshold crossing #theDict['avgWindow_ms': 5 #theDict['dvdt_percentOfMax': 0.1 theDict [ 'halfWidthWindow_ms' ] = 200 #was 20 #theDict['spikeClipWidth_ms': 500 #theDict['onlyPeaksAbove_mV': None #theDict['startSeconds': None #theDict['stopSeconds': None # for detection of Ca from line scans #theDict['caThresholdPos'] = 0.01 #theDict['caMinSpike'] = 0.5 return theDict . copy ()","title":"getDefaultDetection_ca()"},{"location":"bAnalysis/#sanpy.bAnalysis.bAnalysis.getSpikeClips","text":"get 2d list of spike clips, spike clips x, and 1d mean spike clip Parameters: Name Type Description Default theMin float Start seconds. required theMax float Stop seconds. required Source code in sanpy/bAnalysis.py def getSpikeClips ( self , theMin , theMax ): \"\"\" get 2d list of spike clips, spike clips x, and 1d mean spike clip Args: theMin (float): Start seconds. theMax (float): Stop seconds. \"\"\" if theMin is None or theMax is None : theMin = 0 theMax = self . abf . sweepX [ - 1 ] # make a list of clips within start/stop (Seconds) theseClips = [] theseClips_x = [] tmpClips = [] # for mean clip meanClip = [] #if start is not None and stop is not None: for idx , clip in enumerate ( self . spikeClips ): spikeTime = self . spikeTimes [ idx ] spikeTime = self . pnt2Sec_ ( spikeTime ) if spikeTime >= theMin and spikeTime <= theMax : theseClips . append ( clip ) theseClips_x . append ( self . spikeClips_x2 [ idx ]) # remember, all _x are the same if len ( self . spikeClips_x ) == len ( clip ): tmpClips . append ( clip ) # for mean clip if len ( tmpClips ): meanClip = np . mean ( tmpClips , axis = 0 ) return theseClips , theseClips_x , meanClip","title":"getSpikeClips()"},{"location":"bAnalysis/#sanpy.bAnalysis.bAnalysis.getStat","text":"Get a list of values for one or two analysis parameters. For a list of available parameters use xxx. If the returned list of analysis parameters are in points, convert to seconds or ms using: pnt2Sec_(pnt) or pnt2Ms_(pnt). Parameters: Name Type Description Default statName1 str Name of the first analysis parameter to retreive. required statName2 str Optional, Name of the second analysis parameter to retreive. None Returns: Type Description list List of analysis parameter values, None if error. Source code in sanpy/bAnalysis.py def getStat ( self , statName1 , statName2 = None ): \"\"\" Get a list of values for one or two analysis parameters. For a list of available parameters use xxx. If the returned list of analysis parameters are in points, convert to seconds or ms using: pnt2Sec_(pnt) or pnt2Ms_(pnt). Args: statName1 (str): Name of the first analysis parameter to retreive. statName2 (str): Optional, Name of the second analysis parameter to retreive. Returns: list: List of analysis parameter values, None if error. \"\"\" def clean ( val ): if val is None : val = float ( 'nan' ) return val x = None y = None error = False if len ( self . spikeDict ) == 0 : #print('error bAnalysis.getStat(), there are no spikes') error = True elif not statName1 in self . spikeDict [ 0 ] . keys (): print ( 'error: bAnalysis.getStat() did not find statName1' , statName1 , 'in spikeDict' ) error = True elif statName2 is not None and not statName2 in self . spikeDict [ 0 ] . keys (): print ( 'error: bAnalysis.getStat() did not find statName2' , statName2 , 'in spikeDict' ) error = True if not error : x = [ clean ( spike [ statName1 ]) for spike in self . spikeDict ] if statName2 is not None : y = [ clean ( spike [ statName2 ]) for spike in self . spikeDict ] if statName2 is not None : return x , y else : return x","title":"getStat()"},{"location":"bAnalysis/#sanpy.bAnalysis.bAnalysis.getStatMean","text":"Get the mean of an analysis parameter. Parameters: Name Type Description Default statName str Name of the statistic to retreive. For a list of available stats use xxx. required Source code in sanpy/bAnalysis.py def getStatMean ( self , statName ): \"\"\" Get the mean of an analysis parameter. Args: statName (str): Name of the statistic to retreive. For a list of available stats use xxx. \"\"\" theMean = None x = self . getStat ( statName ) if x is not None and len ( x ) > 1 : theMean = np . nanmean ( x ) return theMean","title":"getStatMean()"},{"location":"bAnalysis/#sanpy.bAnalysis.bAnalysis.makeSpikeClips","text":"Parameters: Name Type Description Default spikeClipWidth_ms int Width of each spike clip in milliseconds. required theseTime_sec list of float List of seconds to make clips from. None Returns: Type Description spikeClips_x2 ms spikeClips: sweepY Source code in sanpy/bAnalysis.py def makeSpikeClips ( self , spikeClipWidth_ms , theseTime_sec = None ): \"\"\" Args: spikeClipWidth_ms (int): Width of each spike clip in milliseconds. theseTime_sec (list of float): List of seconds to make clips from. Returns: spikeClips_x2: ms spikeClips: sweepY \"\"\" print ( 'makeSpikeClips() spikeClipWidth_ms:' , spikeClipWidth_ms , 'theseTime_sec:' , theseTime_sec ) if theseTime_sec is None : theseTime_pnts = self . spikeTimes else : # convert theseTime_sec to pnts theseTime_ms = [ x * 1000 for x in theseTime_sec ] theseTime_pnts = [ x * self . abf . dataPointsPerMs for x in theseTime_ms ] theseTime_pnts = [ round ( x ) for x in theseTime_pnts ] clipWidth_pnts = spikeClipWidth_ms * self . abf . dataPointsPerMs clipWidth_pnts = round ( clipWidth_pnts ) if clipWidth_pnts % 2 == 0 : pass # Even else : clipWidth_pnts += 1 # Make odd even halfClipWidth_pnts = int ( clipWidth_pnts / 2 ) print ( ' makeSpikeClips() clipWidth_pnts:' , clipWidth_pnts , 'halfClipWidth_pnts:' , halfClipWidth_pnts ) # make one x axis clip with the threshold crossing at 0 self . spikeClips_x = [( x - halfClipWidth_pnts ) / self . abf . dataPointsPerMs for x in range ( clipWidth_pnts )] #20190714, added this to make all clips same length, much easier to plot in MultiLine numPointsInClip = len ( self . spikeClips_x ) self . spikeClips = [] self . spikeClips_x2 = [] #for idx, spikeTime in enumerate(self.spikeTimes): for idx , spikeTime in enumerate ( theseTime_pnts ): #currentClip = vm[spikeTime-halfClipWidth_pnts:spikeTime+halfClipWidth_pnts] currentClip = self . abf . sweepY [ spikeTime - halfClipWidth_pnts : spikeTime + halfClipWidth_pnts ] if len ( currentClip ) == numPointsInClip : self . spikeClips . append ( currentClip ) self . spikeClips_x2 . append ( self . spikeClips_x ) # a 2D version to make pyqtgraph multiline happy else : pass ## print ( ' ERROR: bAnalysis.spikeDetect() did not add clip for spike index' , idx , 'at time' , spikeTime , 'currentClip:' , len ( currentClip ), 'numPointsInClip:' , numPointsInClip ) ## # return self . spikeClips_x2 , self . spikeClips","title":"makeSpikeClips()"},{"location":"bAnalysis/#sanpy.bAnalysis.bAnalysis.ms2Pnt_","text":"Convert milliseconds (ms) to point in recording using self.abf.dataPointsPerMs Parameters: Name Type Description Default ms float The ms into the recording required Returns: Type Description int The point in the recording Source code in sanpy/bAnalysis.py def ms2Pnt_ ( self , ms ): \"\"\" Convert milliseconds (ms) to point in recording using `self.abf.dataPointsPerMs` Args: ms (float): The ms into the recording Returns: int: The point in the recording \"\"\" theRet = ms * self . abf . dataPointsPerMs theRet = int ( theRet ) return theRet","title":"ms2Pnt_()"},{"location":"bAnalysis/#sanpy.bAnalysis.bAnalysis.numSpikes","text":"Get the number of detected spikes","title":"numSpikes"},{"location":"bAnalysis/#sanpy.bAnalysis.bAnalysis.numSweeps","text":"Get the number of sweeps.","title":"numSweeps"},{"location":"bAnalysis/#sanpy.bAnalysis.bAnalysis.pnt2Ms_","text":"Convert a point to milliseconds (ms) using self.abf.dataPointsPerMs Parameters: Name Type Description Default pnt int The point required Returns: Type Description float The point in milliseconds (ms) Source code in sanpy/bAnalysis.py def pnt2Ms_ ( self , pnt ): \"\"\" Convert a point to milliseconds (ms) using `self.abf.dataPointsPerMs` Args: pnt (int): The point Returns: float: The point in milliseconds (ms) \"\"\" return pnt / self . abf . dataPointsPerMs","title":"pnt2Ms_()"},{"location":"bAnalysis/#sanpy.bAnalysis.bAnalysis.pnt2Sec_","text":"Convert a point to Seconds using self.abf.dataPointsPerMs Parameters: Name Type Description Default pnt int The point required Returns: Type Description float The point in seconds Source code in sanpy/bAnalysis.py def pnt2Sec_ ( self , pnt ): \"\"\" Convert a point to Seconds using `self.abf.dataPointsPerMs` Args: pnt (int): The point Returns: float: The point in seconds \"\"\" if pnt is None : return math . isnan ( pnt ) else : return pnt / self . abf . dataPointsPerMs / 1000","title":"pnt2Sec_()"},{"location":"bAnalysis/#sanpy.bAnalysis.bAnalysis.setSweep","text":"Set the current sweep. Parameters: Name Type Description Default sweepNumber int The sweep number to set. required Source code in sanpy/bAnalysis.py def setSweep ( self , sweepNumber ): \"\"\" Set the current sweep. Args: sweepNumber (int): The sweep number to set. \"\"\" if sweepNumber not in self . abf . sweepList : print ( 'error: bAnalysis.setSweep() did not find sweep' , sweepNumber , ', sweepList =' , self . abf . sweepList ) return False else : self . currentSweep = sweepNumber self . abf . setSweep ( sweepNumber ) return True","title":"setSweep()"},{"location":"bAnalysis/#sanpy.bAnalysis.bAnalysis.spikeDetect","text":"Spike detect the current sweep and put results into self.spikeDict[] . Parameters: Name Type Description Default dDict dict A detection dictionary from bAnalysis.getDefaultDetection() required Source code in sanpy/bAnalysis.py def spikeDetect ( self , dDict , verbose = False ): ''' Spike detect the current sweep and put results into `self.spikeDict[]`. Args: dDict (dict): A detection dictionary from [bAnalysis.getDefaultDetection()][sanpy.bAnalysis.bAnalysis.getDefaultDetection] ''' self . detectionDict = dDict # remember the parameters of our last detection startTime = time . time () if verbose : sanpy . bUtil . printDict ( dDict ) if self . _recordingMode == 'I-Clamp' : self . _getDerivative ( dDict ) elif self . _recordingMode == 'V-Clamp' : self . _getBaselineSubtract ( dDict ) else : # in case recordingMode is ill defined self . _getDerivative ( dDict ) self . spikeDict = [] # we are filling this in, one dict for each spike detectionType = None # # spike detect if dDict [ 'dvdtThreshold' ] is None or np . isnan ( dDict [ 'dvdtThreshold' ]): # detect using mV threshold detectionType = 'mv' self . spikeTimes , spikeErrorList = self . _spikeDetect_vm ( dDict , verbose = verbose ) # backup childish vm threshold if dDict [ 'doBackupSpikeVm' ]: self . spikeTimes = self . _backupSpikeVm ( dDict [ 'medianFilter' ]) else : # detect using dv/dt threshold AND min mV detectionType = 'dvdt' self . spikeTimes , spikeErrorList = self . _spikeDetect_dvdt ( dDict , verbose = verbose ) vm = self . filteredVm dvdt = self . filteredDeriv # # look in a window after each threshold crossing to get AP peak peakWindow_pnts = self . abf . dataPointsPerMs * dDict [ 'peakWindow_ms' ] peakWindow_pnts = round ( peakWindow_pnts ) # # throw out spikes that have peak below onlyPeaksAbove_mV newSpikeTimes = [] newSpikeErrorList = [] if dDict [ 'onlyPeaksAbove_mV' ] is not None : for i , spikeTime in enumerate ( self . spikeTimes ): peakPnt = np . argmax ( vm [ spikeTime : spikeTime + peakWindow_pnts ]) peakPnt += spikeTime peakVal = np . max ( vm [ spikeTime : spikeTime + peakWindow_pnts ]) if peakVal > dDict [ 'onlyPeaksAbove_mV' ]: newSpikeTimes . append ( spikeTime ) newSpikeErrorList . append ( spikeErrorList [ i ]) else : print ( 'spikeDetect() peak height: rejecting spike' , i , 'at pnt:' , spikeTime , \"dDict['onlyPeaksAbove_mV']:\" , dDict [ 'onlyPeaksAbove_mV' ]) # self . spikeTimes = newSpikeTimes spikeErrorList = newSpikeErrorList # # throw out spikes on a down-slope avgWindow_pnts = dDict [ 'avgWindow_ms' ] * self . abf . dataPointsPerMs avgWindow_pnts = math . floor ( avgWindow_pnts / 2 ) for i , spikeTime in enumerate ( self . spikeTimes ): # spikeTime units is ALWAYS points peakPnt = np . argmax ( vm [ spikeTime : spikeTime + peakWindow_pnts ]) peakPnt += spikeTime peakVal = np . max ( vm [ spikeTime : spikeTime + peakWindow_pnts ]) spikeDict = OrderedDict () # use OrderedDict so Pandas output is in the correct order spikeDict [ 'include' ] = 1 spikeDict [ 'analysisVersion' ] = sanpy . analysisVersion spikeDict [ 'interfaceVersion' ] = sanpy . interfaceVersion spikeDict [ 'file' ] = self . file spikeDict [ 'detectionType' ] = detectionType spikeDict [ 'cellType' ] = dDict [ 'cellType' ] spikeDict [ 'sex' ] = dDict [ 'sex' ] spikeDict [ 'condition' ] = dDict [ 'condition' ] spikeDict [ 'spikeNumber' ] = i spikeDict [ 'errors' ] = [] # append existing spikeErrorList from spikeDetect_dvdt() or spikeDetect_mv() tmpError = spikeErrorList [ i ] if tmpError is not None and tmpError != np . nan : #spikeDict['numError'] += 1 spikeDict [ 'errors' ] . append ( tmpError ) # tmpError is from: #eDict = self._getErrorDict(i, spikeTime, errType, errStr) # spikeTime is in pnts # detection params spikeDict [ 'dvdtThreshold' ] = dDict [ 'dvdtThreshold' ] spikeDict [ 'mvThreshold' ] = dDict [ 'mvThreshold' ] spikeDict [ 'medianFilter' ] = dDict [ 'medianFilter' ] spikeDict [ 'halfHeights' ] = dDict [ 'halfHeights' ] spikeDict [ 'thresholdPnt' ] = spikeTime spikeDict [ 'thresholdVal' ] = vm [ spikeTime ] # in vm spikeDict [ 'thresholdVal_dvdt' ] = dvdt [ spikeTime ] # in dvdt spikeDict [ 'thresholdSec' ] = ( spikeTime / self . abf . dataPointsPerMs ) / 1000 spikeDict [ 'peakPnt' ] = peakPnt spikeDict [ 'peakVal' ] = peakVal spikeDict [ 'peakSec' ] = ( peakPnt / self . abf . dataPointsPerMs ) / 1000 spikeDict [ 'peakHeight' ] = spikeDict [ 'peakVal' ] - spikeDict [ 'thresholdVal' ] self . spikeDict . append ( spikeDict ) defaultVal = float ( 'nan' ) # get pre/post spike minima self . spikeDict [ i ][ 'preMinPnt' ] = None self . spikeDict [ i ][ 'preMinVal' ] = defaultVal #self.spikeDict[i]['postMinPnt'] = None #self.spikeDict[i]['postMinVal'] = defaultVal # early diastolic duration # 0.1 to 0.5 of time between pre spike min and spike time self . spikeDict [ i ][ 'preLinearFitPnt0' ] = None self . spikeDict [ i ][ 'preLinearFitPnt1' ] = None self . spikeDict [ i ][ 'earlyDiastolicDuration_ms' ] = defaultVal # seconds between preLinearFitPnt0 and preLinearFitPnt1 self . spikeDict [ i ][ 'preLinearFitVal0' ] = defaultVal self . spikeDict [ i ][ 'preLinearFitVal1' ] = defaultVal # m,b = np.polyfit(x, y, 1) self . spikeDict [ i ][ 'earlyDiastolicDurationRate' ] = defaultVal # fit of y=preLinearFitVal 0/1 versus x=preLinearFitPnt 0/1 self . spikeDict [ i ][ 'lateDiastolicDuration' ] = defaultVal # self . spikeDict [ i ][ 'preSpike_dvdt_max_pnt' ] = None self . spikeDict [ i ][ 'preSpike_dvdt_max_val' ] = defaultVal # in units mV self . spikeDict [ i ][ 'preSpike_dvdt_max_val2' ] = defaultVal # in units dv/dt self . spikeDict [ i ][ 'postSpike_dvdt_min_pnt' ] = None self . spikeDict [ i ][ 'postSpike_dvdt_min_val' ] = defaultVal # in units mV self . spikeDict [ i ][ 'postSpike_dvdt_min_val2' ] = defaultVal # in units dv/dt self . spikeDict [ i ][ 'isi_pnts' ] = defaultVal # time between successive AP thresholds (thresholdSec) self . spikeDict [ i ][ 'isi_ms' ] = defaultVal # time between successive AP thresholds (thresholdSec) self . spikeDict [ i ][ 'spikeFreq_hz' ] = defaultVal # time between successive AP thresholds (thresholdSec) self . spikeDict [ i ][ 'cycleLength_pnts' ] = defaultVal # time between successive MDPs self . spikeDict [ i ][ 'cycleLength_ms' ] = defaultVal # time between successive MDPs # Action potential duration (APD) was defined as the interval between the TOP and the subsequent MDP #self.spikeDict[i]['apDuration_ms'] = defaultVal self . spikeDict [ i ][ 'diastolicDuration_ms' ] = defaultVal # any number of spike widths self . spikeDict [ i ][ 'widths' ] = [] for halfHeight in dDict [ 'halfHeights' ]: widthDict = { 'halfHeight' : halfHeight , 'risingPnt' : None , 'risingVal' : defaultVal , 'fallingPnt' : None , 'fallingVal' : defaultVal , 'widthPnts' : None , 'widthMs' : defaultVal } # abb 20210125, make column width_<n> where <n> is 'halfHeight' self . spikeDict [ i ][ 'widths_' + str ( halfHeight )] = defaultVal # was this self . spikeDict [ i ][ 'widths' ] . append ( widthDict ) # The nonlinear late diastolic depolarization phase was estimated as the duration between 1% and 10% dV/dt # todo: not done !!!!!!!!!! # 20210413, was this. This next block is for pre spike analysis # ok if we are at last spike #if i==0 or i==len(self.spikeTimes)-1: if i == 0 : # was continue but moved half width out of here pass else : mdp_ms = dDict [ 'mdp_ms' ] mdp_pnts = mdp_ms * self . abf . dataPointsPerMs #print('bAnalysis.spikeDetect() needs to be int mdp_pnts:', mdp_pnts) mdp_pnts = int ( mdp_pnts ) # # pre spike min #preRange = vm[self.spikeTimes[i-1]:self.spikeTimes[i]] startPnt = self . spikeTimes [ i ] - mdp_pnts #print(' xxx preRange:', i, startPnt, self.spikeTimes[i]) if startPnt < 0 : # for V-Clammp startPnt = 0 preRange = vm [ startPnt : self . spikeTimes [ i ]] # EXCEPTION preMinPnt = np . argmin ( preRange ) #preMinPnt += self.spikeTimes[i-1] preMinPnt += startPnt # the pre min is actually an average around the real minima avgRange = vm [ preMinPnt - avgWindow_pnts : preMinPnt + avgWindow_pnts ] preMinVal = np . average ( avgRange ) # search backward from spike to find when vm reaches preMinVal (avg) preRange = vm [ preMinPnt : self . spikeTimes [ i ]] preRange = np . flip ( preRange ) # we want to search backwards from peak try : preMinPnt2 = np . where ( preRange < preMinVal )[ 0 ][ 0 ] preMinPnt = self . spikeTimes [ i ] - preMinPnt2 self . spikeDict [ i ][ 'preMinPnt' ] = preMinPnt self . spikeDict [ i ][ 'preMinVal' ] = preMinVal except ( IndexError ) as e : #self.spikeDict[i]['numError'] = self.spikeDict[i]['numError'] + 1 # sometimes preRange is empty, don't try and put min/max in error #print('heroku preMinValue error !!!!!!!!!!!!!!!') #print(' ', self.spikeDict[i]) errorStr = 'searching for preMinVal:' + str ( preMinVal ) #+ ' postRange min:' + str(np.min(postRange)) + ' max ' + str(np.max(postRange)) eDict = self . _getErrorDict ( i , self . spikeTimes [ i ], 'preMin' , errorStr ) # spikeTime is in pnts self . spikeDict [ i ][ 'errors' ] . append ( eDict ) # # linear fit on 10% - 50% of the time from preMinPnt to self.spikeTimes[i] startLinearFit = 0.1 # percent of time between pre spike min and AP peak stopLinearFit = 0.5 # percent of time between pre spike min and AP peak # taking floor() so we always get an integer # points timeInterval_pnts = math . floor ( self . spikeTimes [ i ] - preMinPnt ) preLinearFitPnt0 = preMinPnt + math . floor ( timeInterval_pnts * startLinearFit ) preLinearFitPnt1 = preMinPnt + math . floor ( timeInterval_pnts * stopLinearFit ) preLinearFitVal0 = vm [ preLinearFitPnt0 ] preLinearFitVal1 = vm [ preLinearFitPnt1 ] # linear fit before spike self . spikeDict [ i ][ 'preLinearFitPnt0' ] = preLinearFitPnt0 self . spikeDict [ i ][ 'preLinearFitPnt1' ] = preLinearFitPnt1 self . spikeDict [ i ][ 'earlyDiastolicDuration_ms' ] = self . pnt2Ms_ ( preLinearFitPnt1 - preLinearFitPnt0 ) self . spikeDict [ i ][ 'preLinearFitVal0' ] = preLinearFitVal0 self . spikeDict [ i ][ 'preLinearFitVal1' ] = preLinearFitVal1 # a linear fit where 'm,b = np.polyfit(x, y, 1)' # m*x+b\" xFit = self . abf . sweepX [ preLinearFitPnt0 : preLinearFitPnt1 ] yFit = vm [ preLinearFitPnt0 : preLinearFitPnt1 ] with warnings . catch_warnings (): warnings . filterwarnings ( 'error' ) try : mLinear , bLinear = np . polyfit ( xFit , yFit , 1 ) # m is slope, b is intercept self . spikeDict [ i ][ 'earlyDiastolicDurationRate' ] = mLinear except TypeError : #catching exception: raise TypeError(\"expected non-empty vector for x\") self . spikeDict [ i ][ 'earlyDiastolicDurationRate' ] = defaultVal errorStr = 'earlyDiastolicDurationRate fit' eDict = self . _getErrorDict ( i , self . spikeTimes [ i ], 'fitEDD' , errorStr ) # spikeTime is in pnts self . spikeDict [ i ][ 'errors' ] . append ( eDict ) except np . RankWarning : # also throws: RankWarning: Polyfit may be poorly conditioned self . spikeDict [ i ][ 'earlyDiastolicDurationRate' ] = defaultVal errorStr = 'earlyDiastolicDurationRate fit - RankWarning' eDict = self . _getErrorDict ( i , self . spikeTimes [ i ], 'fitEDD2' , errorStr ) # spikeTime is in pnts self . spikeDict [ i ][ 'errors' ] . append ( eDict ) # not implemented #self.spikeDict[i]['lateDiastolicDuration'] = ??? if True : # # maxima in dv/dt before spike # added try/except sunday april 14, seems to break spike detection??? try : # 20210415 was this #preRange = dvdt[self.spikeTimes[i]:peakPnt] preRange = dvdt [ self . spikeTimes [ i ]: peakPnt + 1 ] preSpike_dvdt_max_pnt = np . argmax ( preRange ) preSpike_dvdt_max_pnt += self . spikeTimes [ i ] self . spikeDict [ i ][ 'preSpike_dvdt_max_pnt' ] = preSpike_dvdt_max_pnt self . spikeDict [ i ][ 'preSpike_dvdt_max_val' ] = vm [ preSpike_dvdt_max_pnt ] # in units mV self . spikeDict [ i ][ 'preSpike_dvdt_max_val2' ] = dvdt [ preSpike_dvdt_max_pnt ] # in units mV except ( ValueError ) as e : #self.spikeDict[i]['numError'] = self.spikeDict[i]['numError'] + 1 # sometimes preRange is empty, don't try and put min/max in error #print('preRange:', preRange) errorStr = 'searching for preSpike_dvdt_max_pnt:' eDict = self . _getErrorDict ( i , self . spikeTimes [ i ], 'preSpikeDvDt' , errorStr ) # spikeTime is in pnts self . spikeDict [ i ][ 'errors' ] . append ( eDict ) # 20210501, we do not need postMin/mdp, not used anywhere else ''' if i==len(self.spikeTimes)-1: # last spike pass else: # # post spike min postRange = vm[self.spikeTimes[i]:self.spikeTimes[i+1]] postMinPnt = np.argmin(postRange) postMinPnt += self.spikeTimes[i] # the post min is actually an average around the real minima avgRange = vm[postMinPnt-avgWindow_pnts:postMinPnt+avgWindow_pnts] postMinVal = np.average(avgRange) # search forward from spike to find when vm reaches postMinVal (avg) postRange = vm[self.spikeTimes[i]:postMinPnt] try: postMinPnt2 = np.where(postRange<postMinVal)[0][0] postMinPnt = self.spikeTimes[i] + postMinPnt2 self.spikeDict[i]['postMinPnt'] = postMinPnt self.spikeDict[i]['postMinVal'] = postMinVal except (IndexError) as e: self.spikeDict[i]['numError'] = self.spikeDict[i]['numError'] + 1 # sometimes postRange is empty, don't try and put min/max in error #print('postRange:', postRange) errorStr = 'searching for postMinVal:' + str(postMinVal) #+ ' postRange min:' + str(np.min(postRange)) + ' max ' + str(np.max(postRange)) eDict = self._getErrorDict(i, self.spikeTimes[i], 'postMinError', errorStr) # spikeTime is in pnts self.spikeDict[i]['errors'].append(eDict) ''' if True : # # minima in dv/dt after spike #postRange = dvdt[self.spikeTimes[i]:postMinPnt] postSpike_ms = 10 postSpike_pnts = self . abf . dataPointsPerMs * postSpike_ms # abb 20210130 lcr analysis postSpike_pnts = round ( postSpike_pnts ) #postRange = dvdt[self.spikeTimes[i]:self.spikeTimes[i]+postSpike_pnts] # fixed window after spike postRange = dvdt [ peakPnt : peakPnt + postSpike_pnts ] # fixed window after spike postSpike_dvdt_min_pnt = np . argmin ( postRange ) postSpike_dvdt_min_pnt += peakPnt self . spikeDict [ i ][ 'postSpike_dvdt_min_pnt' ] = postSpike_dvdt_min_pnt self . spikeDict [ i ][ 'postSpike_dvdt_min_val' ] = vm [ postSpike_dvdt_min_pnt ] self . spikeDict [ i ][ 'postSpike_dvdt_min_val2' ] = dvdt [ postSpike_dvdt_min_pnt ] # 202102 #self.spikeDict[i]['preMinPnt'] = preMinPnt #self.spikeDict[i]['preMinVal'] = preMinVal # 202102 #self.spikeDict[i]['postMinPnt'] = postMinPnt #self.spikeDict[i]['postMinVal'] = postMinVal # linear fit before spike #self.spikeDict[i]['preLinearFitPnt0'] = preLinearFitPnt0 #self.spikeDict[i]['preLinearFitPnt1'] = preLinearFitPnt1 #self.spikeDict[i]['earlyDiastolicDuration_ms'] = self.pnt2Ms_(preLinearFitPnt1 - preLinearFitPnt0) #self.spikeDict[i]['preLinearFitVal0'] = preLinearFitVal0 #self.spikeDict[i]['preLinearFitVal1'] = preLinearFitVal1 # # Action potential duration (APD) was defined as # the interval between the TOP and the subsequent MDP # 20210501, removed AP duration, use APD_90, APD_50 etc ''' if i==len(self.spikeTimes)-1: pass else: self.spikeDict[i]['apDuration_ms'] = self.pnt2Ms_(postMinPnt - spikeDict['thresholdPnt']) ''' # # diastolic duration was defined as # the interval between MDP and TOP if i > 0 : # one off error when preMinPnt is not defined self . spikeDict [ i ][ 'diastolicDuration_ms' ] = self . pnt2Ms_ ( spikeTime - preMinPnt ) self . spikeDict [ i ][ 'cycleLength_ms' ] = float ( 'nan' ) if i > 0 : #20190627, was i>1 isiPnts = self . spikeDict [ i ][ 'thresholdPnt' ] - self . spikeDict [ i - 1 ][ 'thresholdPnt' ] isi_ms = self . pnt2Ms_ ( isiPnts ) isi_hz = 1 / ( isi_ms / 1000 ) self . spikeDict [ i ][ 'isi_pnts' ] = isiPnts self . spikeDict [ i ][ 'isi_ms' ] = self . pnt2Ms_ ( isiPnts ) self . spikeDict [ i ][ 'spikeFreq_hz' ] = 1 / ( self . pnt2Ms_ ( isiPnts ) / 1000 ) # Cycle length was defined as the interval between MDPs in successive APs prevPreMinPnt = self . spikeDict [ i - 1 ][ 'preMinPnt' ] # can be nan thisPreMinPnt = self . spikeDict [ i ][ 'preMinPnt' ] if prevPreMinPnt is not None and thisPreMinPnt is not None : cycleLength_pnts = thisPreMinPnt - prevPreMinPnt self . spikeDict [ i ][ 'cycleLength_pnts' ] = cycleLength_pnts self . spikeDict [ i ][ 'cycleLength_ms' ] = self . pnt2Ms_ ( cycleLength_pnts ) else : # error #self.spikeDict[i]['numError'] = self.spikeDict[i]['numError'] + 1 errorStr = 'previous spike preMinPnt is ' + str ( prevPreMinPnt ) + ' this preMinPnt:' + str ( thisPreMinPnt ) eDict = self . _getErrorDict ( i , self . spikeTimes [ i ], 'cycleLength' , errorStr ) # spikeTime is in pnts self . spikeDict [ i ][ 'errors' ] . append ( eDict ) ''' # 20210501 was this, I am no longer using postMinPnt prevPostMinPnt = self.spikeDict[i-1]['postMinPnt'] tmpPostMinPnt = self.spikeDict[i]['postMinPnt'] if prevPostMinPnt is not None and tmpPostMinPnt is not None: cycleLength_pnts = tmpPostMinPnt - prevPostMinPnt self.spikeDict[i]['cycleLength_pnts'] = cycleLength_pnts self.spikeDict[i]['cycleLength_ms'] = self.pnt2Ms_(cycleLength_pnts) else: self.spikeDict[i]['numError'] = self.spikeDict[i]['numError'] + 1 errorStr = 'previous spike postMinPnt is ' + str(prevPostMinPnt) + ' this postMinPnt:' + str(tmpPostMinPnt) eDict = self._getErrorDict(i, self.spikeTimes[i], 'cycleLength', errorStr) # spikeTime is in pnts self.spikeDict[i]['errors'].append(eDict) ''' # # spike half with and APDur # # # 20210130, moving 'half width' out of inner if spike # is first/last # # get 1/2 height (actually, any number of height measurements) # action potential duration using peak and post min #self.spikeDict[i]['widths'] = [] #print('*** calculating half width for spike', i) doWidthVersion2 = False #halfWidthWindow_ms = 20 hwWindowPnts = dDict [ 'halfWidthWindow_ms' ] * self . abf . dataPointsPerMs hwWindowPnts = round ( hwWindowPnts ) tmpPeakSec = spikeDict [ 'peakSec' ] tmpErrorType = None for j , halfHeight in enumerate ( dDict [ 'halfHeights' ]): # halfHeight in [20, 50, 80] if doWidthVersion2 : tmpThreshVm = spikeDict [ 'thresholdVal' ] thisVm = tmpThreshVm + ( peakVal - tmpThreshVm ) * ( halfHeight * 0.01 ) else : # 20210413 was this #thisVm = postMinVal + (peakVal - postMinVal) * (halfHeight * 0.01) tmpThreshVm2 = spikeDict [ 'thresholdVal' ] thisVm = tmpThreshVm2 + ( peakVal - tmpThreshVm2 ) * ( halfHeight * 0.01 ) #todo: logic is broken, this get over-written in following try widthDict = { 'halfHeight' : halfHeight , 'risingPnt' : None , 'risingVal' : defaultVal , 'fallingPnt' : None , 'fallingVal' : defaultVal , 'widthPnts' : None , 'widthMs' : defaultVal } widthMs = np . nan try : if doWidthVersion2 : postRange = vm [ peakPnt : peakPnt + hwWindowPnts ] else : # 20210413 was this #postRange = vm[peakPnt:postMinPnt] postRange = vm [ peakPnt : peakPnt + hwWindowPnts ] fallingPnt = np . where ( postRange < thisVm )[ 0 ] # less than if len ( fallingPnt ) == 0 : #error tmpErrorType = 'falling point' raise IndexError fallingPnt = fallingPnt [ 0 ] # first falling point fallingPnt += peakPnt fallingVal = vm [ fallingPnt ] # use the post/falling to find pre/rising if doWidthVersion2 : preRange = vm [ peakPnt - hwWindowPnts : peakPnt ] else : tmpPreMinPnt2 = spikeDict [ 'thresholdPnt' ] preRange = vm [ tmpPreMinPnt2 : peakPnt ] risingPnt = np . where ( preRange > fallingVal )[ 0 ] # greater than if len ( risingPnt ) == 0 : #error tmpErrorType = 'rising point' raise IndexError risingPnt = risingPnt [ 0 ] # first falling point if doWidthVersion2 : risingPnt += peakPnt - hwWindowPnts else : risingPnt += spikeDict [ 'thresholdPnt' ] risingVal = vm [ risingPnt ] # width (pnts) widthPnts = fallingPnt - risingPnt # 20210413 widthPnts2 = fallingPnt - spikeDict [ 'thresholdPnt' ] tmpRisingPnt = spikeDict [ 'thresholdPnt' ] # assign widthDict [ 'halfHeight' ] = halfHeight # 20210413, put back in #widthDict['risingPnt'] = risingPnt widthDict [ 'risingPnt' ] = tmpRisingPnt widthDict [ 'risingVal' ] = risingVal widthDict [ 'fallingPnt' ] = fallingPnt widthDict [ 'fallingVal' ] = fallingVal widthDict [ 'widthPnts' ] = widthPnts widthDict [ 'widthMs' ] = widthPnts / self . abf . dataPointsPerMs widthMs = widthPnts / self . abf . dataPointsPerMs # abb 20210125 # 20210413, todo: make these end in 2 widthDict [ 'widthPnts' ] = widthPnts2 widthDict [ 'widthMs' ] = widthPnts / self . abf . dataPointsPerMs except ( IndexError ) as e : ## ## #print(' EXCEPTION: bAnalysis.spikeDetect() spike', i, 'half height', halfHeight) ## ## #self.spikeDict[i]['numError'] = self.spikeDict[i]['numError'] + 1 #errorStr = 'spike ' + str(i) + ' half width ' + str(tmpErrorType) + ' ' + str(halfHeight) + ' halfWidthWindow_ms:' + str(dDict['halfWidthWindow_ms']) errorStr = ( f 'half width { halfHeight } error in { tmpErrorType } ' f \"with halfWidthWindow_ms: { dDict [ 'halfWidthWindow_ms' ] } \" f 'searching for Vm: { round ( thisVm , 2 ) } from peak sec { round ( tmpPeakSec , 2 ) } ' ) eDict = self . _getErrorDict ( i , self . spikeTimes [ i ], 'spikeWidth' , errorStr ) # spikeTime is in pnts self . spikeDict [ i ][ 'errors' ] . append ( eDict ) # abb 20210125 # wtf is hapenning on Heroku???? #print('**** heroku debug i:', i, 'j:', j, 'len:', len(self.spikeDict), 'halfHeight:', halfHeight) #print(' self.spikeDict[i]', self.spikeDict[i]['widths_'+str(halfHeight)]) self . spikeDict [ i ][ 'widths_' + str ( halfHeight )] = widthMs self . spikeDict [ i ][ 'widths' ][ j ] = widthDict # # look between threshold crossing to get minima # we will ignore the first and last spike # todo: call self.makeSpikeClips() # # build a list of spike clips #clipWidth_ms = 500 clipWidth_pnts = dDict [ 'spikeClipWidth_ms' ] * self . abf . dataPointsPerMs clipWidth_pnts = round ( clipWidth_pnts ) if clipWidth_pnts % 2 == 0 : pass # Even else : clipWidth_pnts += 1 # Odd halfClipWidth_pnts = int ( clipWidth_pnts / 2 ) print ( ' spikeDetect() clipWidth_pnts:' , clipWidth_pnts , 'halfClipWidth_pnts:' , halfClipWidth_pnts ) # make one x axis clip with the threshold crossing at 0 self . spikeClips_x = [( x - halfClipWidth_pnts ) / self . abf . dataPointsPerMs for x in range ( clipWidth_pnts )] #20190714, added this to make all clips same length, much easier to plot in MultiLine numPointsInClip = len ( self . spikeClips_x ) self . spikeClips = [] self . spikeClips_x2 = [] for idx , spikeTime in enumerate ( self . spikeTimes ): #currentClip = vm[spikeTime-halfClipWidth_pnts:spikeTime+halfClipWidth_pnts] currentClip = vm [ spikeTime - halfClipWidth_pnts : spikeTime + halfClipWidth_pnts ] if len ( currentClip ) == numPointsInClip : self . spikeClips . append ( currentClip ) self . spikeClips_x2 . append ( self . spikeClips_x ) # a 2D version to make pyqtgraph multiline happy else : ## ## if idx == 0 or idx == len ( self . spikeTimes ) - 1 : # don't report spike clip errors for first/last spike pass else : print ( ' ERROR: bAnalysis.spikeDetect() did not add clip for spike index' , idx , 'at time' , spikeTime , 'currentClip:' , len ( currentClip ), 'numPointsInClip:' , numPointsInClip ) ## ## # 20210426 # generate a df holding stats (used by scatterplotwidget) print ( ' making df to pass to scatterplotwidget' ) startSeconds = dDict [ 'startSeconds' ] stopSeconds = dDict [ 'stopSeconds' ] #tmpAnalysisName, df0 = self.getReportDf(theMin, theMax, savefile) exportObject = sanpy . bExport ( self ) dfReportForScatter = exportObject . report ( startSeconds , stopSeconds ) stopTime = time . time () if verbose : print ( 'bAnalysis.spikeDetect() for file' , self . file ) print ( ' detected' , len ( self . spikeTimes ), 'spikes in' , round ( stopTime - startTime , 2 ), 'seconds' ) self . errorReport () return dfReportForScatter","title":"spikeDetect()"},{"location":"bAnalysis/#sanpy.bAnalysis.bAnalysis.sweepList","text":"Get a list of sweeps.","title":"sweepList"},{"location":"bExport/","text":"bExport \u00a4 Once analysis is performed with sanpy.bAnalysis.spikeDetect(dDict), reports can be generated with the bExport class. Example reports are: Generating reports as a Pandas DataFrame. Saving reports as a Microsoft Excel file. Saving reports as a CSV text files. __init__ ( self , ba ) special \u00a4 Parameters: Name Type Description Default ba sanpy.bAnalysis A bAnalysis object that has had spikes detected with detectSpikes(). required Source code in sanpy/bExport.py def __init__ ( self , ba ): \"\"\" Args: ba (sanpy.bAnalysis): A bAnalysis object that has had spikes detected with detectSpikes(). \"\"\" self . ba = ba getReportDf ( self , theMin , theMax , savefile ) \u00a4 Get spikes as a Pandas DataFrame, one row per spike. Parameters: Name Type Description Default theMin float xxx required theMax float xxx required savefile str .xls file path required Returns: Type Description df Pandas DataFrame Source code in sanpy/bExport.py def getReportDf ( self , theMin , theMax , savefile ): \"\"\" Get spikes as a Pandas DataFrame, one row per spike. Args: theMin (float): xxx theMax (float): xxx savefile (str): .xls file path Returns: df: Pandas DataFrame \"\"\" filePath , fileName = os . path . split ( os . path . abspath ( savefile )) # make an analysis folder filePath = os . path . join ( filePath , 'analysis' ) if not os . path . isdir ( filePath ): print ( ' getReportDf() making output folder:' , filePath ) os . mkdir ( filePath ) textFileBaseName , tmpExtension = os . path . splitext ( fileName ) textFilePath = os . path . join ( filePath , textFileBaseName + '.csv' ) # save header textFileHeader = OrderedDict () textFileHeader [ 'file' ] = self . ba . file # this is actuall file path #textFileHeader['condition1'] = self.ba.condition1 #textFileHeader['condition2'] = self.ba.condition2 #textFileHeader['condition3'] = self.ba.condition3 textFileHeader [ 'cellType' ] = self . ba . detectiondict [ 'cellType' ] textFileHeader [ 'sex' ] = self . ba . detectiondict [ 'sex' ] textFileHeader [ 'condition' ] = self . ba . detectiondict [ 'condition' ] # textFileHeader [ 'dateAnalyzed' ] = self . ba . dateAnalyzed textFileHeader [ 'detectionType' ] = self . ba . detectionType textFileHeader [ 'dvdtThreshold' ] = [ self . ba . detectionDict [ 'dvdtThreshold' ]] textFileHeader [ 'mvThreshold' ] = [ self . ba . detectionDict [ 'mvThreshold' ]] #textFileHeader['medianFilter'] = self.ba.medianFilter textFileHeader [ 'startSeconds' ] = ' %.2f ' % ( theMin ) textFileHeader [ 'stopSeconds' ] = ' %.2f ' % ( theMax ) #textFileHeader['startSeconds'] = self.ba.startSeconds #textFileHeader['stopSeconds'] = self.ba.stopSeconds textFileHeader [ 'currentSweep' ] = self . ba . currentSweep textFileHeader [ 'numSweeps' ] = self . ba . numSweeps #textFileHeader['theMin'] = theMin #textFileHeader['theMax'] = theMax # 20210125, this is not needed, we are saviing pandas df below ??? headerStr = '' for k , v in textFileHeader . items (): headerStr += k + '=' + str ( v ) + ';' headerStr += ' \\n ' #print('headerStr:', headerStr) with open ( textFilePath , 'w' ) as f : f . write ( headerStr ) #print('Saving .txt file:', textFilePath) df = self . report ( theMin , theMax ) # we need a column indicating (path), the original .abf file # along with (start,stop) which should make this analysis unique? minStr = ' %.2f ' % ( theMin ) maxStr = ' %.2f ' % ( theMax ) minStr = minStr . replace ( '.' , '_' ) maxStr = maxStr . replace ( '.' , '_' ) tmpPath , tmpFile = os . path . split ( self . ba . file ) tmpFile , tmpExt = os . path . splitext ( tmpFile ) analysisName = tmpFile + '_s' + minStr + '_s' + maxStr print ( ' minStr:' , minStr , 'maxStr:' , maxStr , 'analysisName:' , analysisName ) df [ 'analysisname' ] = analysisName # should be filled in by self.ba.report #df['Condition'] = df['condition1'] #df['File Number'] = df['condition2'] #df['Sex'] = df['condition3'] #df['Region'] = df['condition4'] df [ 'filename' ] = [ os . path . splitext ( os . path . split ( x )[ 1 ])[ 0 ] for x in df [ 'file' ] . tolist ()] # print ( ' bExport.getReportDf() saving text file:' , textFilePath ) #df.to_csv(textFilePath, sep=',', index_label='index', mode='a') df . to_csv ( textFilePath , sep = ',' , index_label = 'index' , mode = 'w' ) return analysisName , df report ( self , theMin , theMax ) \u00a4 Get entire spikeDict as a Pandas DataFrame. Parameters: Name Type Description Default theMin float Start seconds of the analysis required theMax float Stop seconds of the analysis required Returns: Type Description df Pandas DataFrame Source code in sanpy/bExport.py def report ( self , theMin , theMax ): \"\"\" Get entire spikeDict as a Pandas DataFrame. Args: theMin (float): Start seconds of the analysis theMax (float): Stop seconds of the analysis Returns: df: Pandas DataFrame \"\"\" if theMin is None or theMax is None : return None df = pd . DataFrame ( self . ba . spikeDict ) df = df [ df [ 'thresholdSec' ] . between ( theMin , theMax , inclusive = True )] # added when trying to make scatterwidget for one file #print(' 20210426 adding columns in bExport.report()') #df['Condition'] = df['condition1'] #df['File Number'] = df['condition2'] #df['Sex'] = df['condition3'] #df['Region'] = df['condition4'] # make new column with sex/region encoded ''' tmpNewCol = 'RegSex' self.ba.masterDf[tmpNewCol] = '' for tmpRegion in ['Superior', 'Inferior']: for tmpSex in ['Male', 'Female']: newEncoding = tmpRegion[0] + tmpSex[0] regSex = self.ba.masterDf[ (self.ba.masterDf['Region']==tmpRegion) & (self.ba.masterDf['Sex']==tmpSex)] regSex = (self.ba.masterDf['Region']==tmpRegion) & (self.ba.masterDf['Sex']==tmpSex) print('newEncoding:', newEncoding, 'regSex:', regSex.shape) self.ba.masterDf.loc[regSex, tmpNewCol] = newEncoding ''' # want this but region/sex/condition are not defined print ( 'bExport.report()' ) print ( df . head ()) tmpNewCol = 'CellTypeSex' cellTypeStr = df [ 'cellType' ] . iloc [ 0 ] sexStr = df [ 'sex' ] . iloc [ 0 ] print ( 'cellTypeStr:' , cellTypeStr , 'sexStr:' , sexStr ) regSexEncoding = cellTypeStr + sexStr df [ tmpNewCol ] = regSexEncoding minStr = ' %.2f ' % ( theMin ) maxStr = ' %.2f ' % ( theMax ) minStr = minStr . replace ( '.' , '_' ) maxStr = maxStr . replace ( '.' , '_' ) tmpPath , tmpFile = os . path . split ( self . ba . file ) tmpFile , tmpExt = os . path . splitext ( tmpFile ) analysisName = tmpFile + '_s' + minStr + '_s' + maxStr print ( ' minStr:' , minStr , 'maxStr:' , maxStr , 'analysisName:' , analysisName ) df [ 'analysisname' ] = analysisName return df report2 ( self , theMin , theMax ) \u00a4 Generate a report of spikes with spike times between theMin (sec) and theMax (sec). Parameters: Name Type Description Default theMin float Start seconds to save required theMax float Stop seconds to save required Returns: Type Description df Pandas DataFrame Source code in sanpy/bExport.py def report2 ( self , theMin , theMax ): \"\"\" Generate a report of spikes with spike times between theMin (sec) and theMax (sec). Args: theMin (float): Start seconds to save theMax (float): Stop seconds to save Returns: df: Pandas DataFrame \"\"\" newList = [] for spike in self . ba . spikeDict : # if current spike time is out of bounds then continue (e.g. it is not between theMin (sec) and theMax (sec) spikeTime_sec = self . ba . pnt2Sec_ ( spike [ 'thresholdPnt' ]) if spikeTime_sec < theMin or spikeTime_sec > theMax : continue spikeDict = OrderedDict () # use OrderedDict so Pandas output is in the correct order spikeDict [ 'Take Off Potential (s)' ] = self . ba . pnt2Sec_ ( spike [ 'thresholdPnt' ]) spikeDict [ 'Take Off Potential (ms)' ] = self . ba . pnt2Ms_ ( spike [ 'thresholdPnt' ]) spikeDict [ 'Take Off Potential (mV)' ] = spike [ 'thresholdVal' ] spikeDict [ 'AP Peak (ms)' ] = self . ba . pnt2Ms_ ( spike [ 'peakPnt' ]) spikeDict [ 'AP Peak (mV)' ] = spike [ 'peakVal' ] spikeDict [ 'AP Height (mV)' ] = spike [ 'peakHeight' ] spikeDict [ 'Pre AP Min (mV)' ] = spike [ 'preMinVal' ] #spikeDict['Post AP Min (mV)'] = spike['postMinVal'] # #spikeDict['AP Duration (ms)'] = spike['apDuration_ms'] spikeDict [ 'Early Diastolic Duration (ms)' ] = spike [ 'earlyDiastolicDuration_ms' ] spikeDict [ 'Early Diastolic Depolarization Rate (dV/s)' ] = spike [ 'earlyDiastolicDurationRate' ] # abb 202012 spikeDict [ 'Diastolic Duration (ms)' ] = spike [ 'diastolicDuration_ms' ] # spikeDict [ 'Inter-Spike-Interval (ms)' ] = spike [ 'isi_ms' ] spikeDict [ 'Spike Frequency (Hz)' ] = spike [ 'spikeFreq_hz' ] spikeDict [ 'Cycle Length (ms)' ] = spike [ 'cycleLength_ms' ] spikeDict [ 'Max AP Upstroke (dV/dt)' ] = spike [ 'preSpike_dvdt_max_val2' ] spikeDict [ 'Max AP Upstroke (mV)' ] = spike [ 'preSpike_dvdt_max_val' ] spikeDict [ 'Max AP Repolarization (dV/dt)' ] = spike [ 'postSpike_dvdt_min_val2' ] spikeDict [ 'Max AP Repolarization (mV)' ] = spike [ 'postSpike_dvdt_min_val' ] # half-width for widthDict in spike [ 'widths' ]: keyName = 'width_' + str ( widthDict [ 'halfHeight' ]) spikeDict [ keyName ] = widthDict [ 'widthMs' ] # errors #spikeDict['numError'] = spike['numError'] spikeDict [ 'errors' ] = spike [ 'errors' ] # append newList . append ( spikeDict ) df = pd . DataFrame ( newList ) return df saveReport ( self , savefile , theMin = None , theMax = None , saveExcel = True , alsoSaveTxt = True , verbose = True ) \u00a4 Save a spike report for detected spikes between theMin (sec) and theMax (sec) Parameters: Name Type Description Default savefile str path to xlsx file required theMin float start/stop seconds of the analysis None theMax float start/stop seconds of the analysis None saveExcel bool xxx True alsoSaveTxt bool xxx True Returns: Type Description str analysisName df: df Source code in sanpy/bExport.py def saveReport ( self , savefile , theMin = None , theMax = None , saveExcel = True , alsoSaveTxt = True , verbose = True ): \"\"\" Save a spike report for detected spikes between theMin (sec) and theMax (sec) Args: savefile (str): path to xlsx file theMin (float): start/stop seconds of the analysis theMax (float): start/stop seconds of the analysis saveExcel (bool): xxx alsoSaveTxt (bool): xxx Return: str: analysisName df: df \"\"\" if theMin == None : theMin = 0 if theMax == None : theMax = self . ba . abf . sweepX [ - 1 ] # always grab a df to the entire analysis (not sure what I will do with this) #df = self.ba.report() # report() is my own 'bob' verbiage theRet = None if saveExcel and savefile : if verbose : print ( ' bExport.saveReport() saving user specified .xlsx file:' , savefile ) excelFilePath = savefile writer = pd . ExcelWriter ( excelFilePath , engine = 'xlsxwriter' ) # # cardiac style analysis to sheet 'cardiac' cardiac_df = self . report2 ( theMin , theMax ) # report2 is more 'cardiac' # # header sheet headerDict = OrderedDict () filePath , fileName = os . path . split ( self . ba . file ) headerDict [ 'File Name' ] = [ fileName ] headerDict [ 'File Path' ] = [ filePath ] headerDict [ 'Cell Type' ] = [ self . ba . detectionDict [ 'cellType' ]] headerDict [ 'Sex' ] = [ self . ba . detectionDict [ 'sex' ]] headerDict [ 'Condition' ] = [ self . ba . detectionDict [ 'condition' ]] # todo: get these params in ONE dict inside self.ba dateAnalyzed , timeAnalyzed = self . ba . dateAnalyzed . split ( ' ' ) headerDict [ 'Date Analyzed' ] = [ dateAnalyzed ] headerDict [ 'Time Analyzed' ] = [ timeAnalyzed ] headerDict [ 'Detection Type' ] = [ self . ba . detectionType ] headerDict [ 'dV/dt Threshold' ] = [ self . ba . detectionDict [ 'dvdtThreshold' ]] #headerDict['mV Threshold'] = [self.ba.mvThreshold] # abb 202012 headerDict [ 'Vm Threshold (mV)' ] = [ self . ba . detectionDict [ 'mvThreshold' ]] #headerDict['Median Filter (pnts)'] = [self.ba.medianFilter] headerDict [ 'Analysis Version' ] = [ sanpy . analysisVersion ] headerDict [ 'Interface Version' ] = [ sanpy . interfaceVersion ] #headerDict['Analysis Start (sec)'] = [self.ba.startSeconds] #headerDict['Analysis Stop (sec)'] = [self.ba.stopSeconds] headerDict [ 'Sweep Number' ] = [ self . ba . currentSweep ] headerDict [ 'Number of Sweeps' ] = [ self . ba . numSweeps ] headerDict [ 'Export Start (sec)' ] = [ float ( ' %.2f ' % ( theMin ))] # on export, x-axis of raw plot will be ouput headerDict [ 'Export Stop (sec)' ] = [ float ( ' %.2f ' % ( theMax ))] # on export, x-axis of raw plot will be ouput # 'stats' has xxx columns (name, mean, sd, se, n) headerDict [ 'stats' ] = [] for idx , col in enumerate ( cardiac_df ): headerDict [ col ] = [] # mean theMean = cardiac_df . mean () # skipna default is True theMean [ 'errors' ] = '' # sd theSD = cardiac_df . std () # skipna default is True theSD [ 'errors' ] = '' #se theSE = cardiac_df . sem () # skipna default is True theSE [ 'errors' ] = '' #n theN = cardiac_df . count () # skipna default is True theN [ 'errors' ] = '' statCols = [ 'mean' , 'sd' , 'se' , 'n' ] for j , stat in enumerate ( statCols ): if j == 0 : pass else : # need to append columns to keep Excel sheet columns in sync #for k,v in headerDict.items(): # headerDict[k].append('') headerDict [ 'File Name' ] . append ( '' ) headerDict [ 'File Path' ] . append ( '' ) headerDict [ 'Cell Type' ] . append ( '' ) headerDict [ 'Sex' ] . append ( '' ) headerDict [ 'Condition' ] . append ( '' ) # headerDict [ 'Date Analyzed' ] . append ( '' ) headerDict [ 'Time Analyzed' ] . append ( '' ) headerDict [ 'Detection Type' ] . append ( '' ) headerDict [ 'dV/dt Threshold' ] . append ( '' ) headerDict [ 'Vm Threshold (mV)' ] . append ( '' ) #headerDict['Median Filter (pnts)'].append('') headerDict [ 'Analysis Version' ] . append ( '' ) headerDict [ 'Interface Version' ] . append ( '' ) headerDict [ 'Sweep Number' ] . append ( '' ) headerDict [ 'Number of Sweeps' ] . append ( '' ) headerDict [ 'Export Start (sec)' ] . append ( '' ) headerDict [ 'Export Stop (sec)' ] . append ( '' ) # a dictionary key for each stat headerDict [ 'stats' ] . append ( stat ) for idx , col in enumerate ( cardiac_df ): #headerDict[col].append('') if stat == 'mean' : headerDict [ col ] . append ( theMean [ col ]) elif stat == 'sd' : headerDict [ col ] . append ( theSD [ col ]) elif stat == 'se' : headerDict [ col ] . append ( theSE [ col ]) elif stat == 'n' : headerDict [ col ] . append ( theN [ col ]) #print(headerDict) #for k,v in headerDict.items(): # print(k, v) # dict to pandas dataframe df = pd . DataFrame ( headerDict ) . T df . to_excel ( writer , sheet_name = 'summary' ) # set the column widths in excel sheet 'cardiac' columnWidth = 25 worksheet = writer . sheets [ 'summary' ] # pull worksheet object for idx , col in enumerate ( df ): # loop through all columns worksheet . set_column ( idx , idx , columnWidth ) # set column width # # 'params' sheet with all detection params # need to convert list values in dict to string (o.w. we get one row per item in list) exportDetectionDict = {} for k , v in self . ba . detectionDict . items (): if isinstance ( v , list ): v = f '\" { v } \"' exportDetectionDict [ k ] = v #print(' === \"params\" sheet exportDetectionDict:', exportDetectionDict) df = pd . DataFrame ( exportDetectionDict , index = [ 0 ]) . T # index=[0] needed when dict has all scalar values #print(' df:') #print(df) df . to_excel ( writer , sheet_name = 'params' ) columnWidth = 25 worksheet = writer . sheets [ 'params' ] # pull worksheet object worksheet . set_column ( 0 , 0 , columnWidth ) # set column width # # 'cardiac' sheet cardiac_df . to_excel ( writer , sheet_name = 'cardiac' ) # set the column widths in excel sheet 'cardiac' columnWidth = 20 worksheet = writer . sheets [ 'cardiac' ] # pull worksheet object for idx , col in enumerate ( cardiac_df ): # loop through all columns worksheet . set_column ( idx , idx , columnWidth ) # set column width # # entire (verbose) analysis to sheet 'bob' #df.to_excel(writer, sheet_name='bob') # # mean spike clip theseClips , theseClips_x , meanClip = self . ba . getSpikeClips ( theMin , theMax ) try : first_X = theseClips_x [ 0 ] #- theseClips_x[0][0] #if verbose: print(' bExport.saveReport() saving mean clip to sheet \"Avg Spike\" from', len(theseClips), 'clips') df = pd . DataFrame ( meanClip , first_X ) df . to_excel ( writer , sheet_name = 'Avg Spike' ) except ( IndexError ) as e : print ( 'warning: got bad spike clips in saveReport(). Usually happend when 1-2 spikes' ) #print('df:', df) writer . save () # # save a csv text file # analysisName = '' if alsoSaveTxt : # this also saves analysisName , df0 = self . getReportDf ( theMin , theMax , savefile ) # # save mean spike clip theseClips , theseClips_x , meanClip = self . ba . getSpikeClips ( theMin , theMax ) if len ( theseClips_x ) == 0 : pass else : first_X = theseClips_x [ 0 ] #- theseClips_x[0][0] first_X = np . array ( first_X ) first_X /= self . ba . abf . dataPointsPerMs # pnts to ms #if verbose: print(' bExport.saveReport() saving mean clip to sheet \"Avg Spike\" from', len(theseClips), 'clips') #dfClip = pd.DataFrame(meanClip, first_X) dfClip = pd . DataFrame . from_dict ({ 'xMs' : first_X , 'yVm' : meanClip }) # load clip based on analysisname (with start/stop seconds) analysisname = df0 [ 'analysisname' ] . iloc [ 0 ] # name with start/stop seconds print ( 'bExport.saveReport() analysisname:' , analysisname ) #print('analysisname:', analysisname) clipFileName = analysisname + '_clip.csv' tmpPath , tmpFile = os . path . split ( savefile ) tmpPath = os . path . join ( tmpPath , 'analysis' ) # dir is already created in getReportDf if not os . path . isdir ( tmpPath ): os . mkdir ( tmpPath ) clipSavePath = os . path . join ( tmpPath , clipFileName ) print ( ' clipSavePath:' , clipSavePath ) dfClip . to_csv ( clipSavePath ) # theRet = df0 # return analysisName , theRet","title":"bExport"},{"location":"bExport/#sanpy.bExport.bExport","text":"Once analysis is performed with sanpy.bAnalysis.spikeDetect(dDict), reports can be generated with the bExport class. Example reports are: Generating reports as a Pandas DataFrame. Saving reports as a Microsoft Excel file. Saving reports as a CSV text files.","title":"bExport"},{"location":"bExport/#sanpy.bExport.bExport.__init__","text":"Parameters: Name Type Description Default ba sanpy.bAnalysis A bAnalysis object that has had spikes detected with detectSpikes(). required Source code in sanpy/bExport.py def __init__ ( self , ba ): \"\"\" Args: ba (sanpy.bAnalysis): A bAnalysis object that has had spikes detected with detectSpikes(). \"\"\" self . ba = ba","title":"__init__()"},{"location":"bExport/#sanpy.bExport.bExport.getReportDf","text":"Get spikes as a Pandas DataFrame, one row per spike. Parameters: Name Type Description Default theMin float xxx required theMax float xxx required savefile str .xls file path required Returns: Type Description df Pandas DataFrame Source code in sanpy/bExport.py def getReportDf ( self , theMin , theMax , savefile ): \"\"\" Get spikes as a Pandas DataFrame, one row per spike. Args: theMin (float): xxx theMax (float): xxx savefile (str): .xls file path Returns: df: Pandas DataFrame \"\"\" filePath , fileName = os . path . split ( os . path . abspath ( savefile )) # make an analysis folder filePath = os . path . join ( filePath , 'analysis' ) if not os . path . isdir ( filePath ): print ( ' getReportDf() making output folder:' , filePath ) os . mkdir ( filePath ) textFileBaseName , tmpExtension = os . path . splitext ( fileName ) textFilePath = os . path . join ( filePath , textFileBaseName + '.csv' ) # save header textFileHeader = OrderedDict () textFileHeader [ 'file' ] = self . ba . file # this is actuall file path #textFileHeader['condition1'] = self.ba.condition1 #textFileHeader['condition2'] = self.ba.condition2 #textFileHeader['condition3'] = self.ba.condition3 textFileHeader [ 'cellType' ] = self . ba . detectiondict [ 'cellType' ] textFileHeader [ 'sex' ] = self . ba . detectiondict [ 'sex' ] textFileHeader [ 'condition' ] = self . ba . detectiondict [ 'condition' ] # textFileHeader [ 'dateAnalyzed' ] = self . ba . dateAnalyzed textFileHeader [ 'detectionType' ] = self . ba . detectionType textFileHeader [ 'dvdtThreshold' ] = [ self . ba . detectionDict [ 'dvdtThreshold' ]] textFileHeader [ 'mvThreshold' ] = [ self . ba . detectionDict [ 'mvThreshold' ]] #textFileHeader['medianFilter'] = self.ba.medianFilter textFileHeader [ 'startSeconds' ] = ' %.2f ' % ( theMin ) textFileHeader [ 'stopSeconds' ] = ' %.2f ' % ( theMax ) #textFileHeader['startSeconds'] = self.ba.startSeconds #textFileHeader['stopSeconds'] = self.ba.stopSeconds textFileHeader [ 'currentSweep' ] = self . ba . currentSweep textFileHeader [ 'numSweeps' ] = self . ba . numSweeps #textFileHeader['theMin'] = theMin #textFileHeader['theMax'] = theMax # 20210125, this is not needed, we are saviing pandas df below ??? headerStr = '' for k , v in textFileHeader . items (): headerStr += k + '=' + str ( v ) + ';' headerStr += ' \\n ' #print('headerStr:', headerStr) with open ( textFilePath , 'w' ) as f : f . write ( headerStr ) #print('Saving .txt file:', textFilePath) df = self . report ( theMin , theMax ) # we need a column indicating (path), the original .abf file # along with (start,stop) which should make this analysis unique? minStr = ' %.2f ' % ( theMin ) maxStr = ' %.2f ' % ( theMax ) minStr = minStr . replace ( '.' , '_' ) maxStr = maxStr . replace ( '.' , '_' ) tmpPath , tmpFile = os . path . split ( self . ba . file ) tmpFile , tmpExt = os . path . splitext ( tmpFile ) analysisName = tmpFile + '_s' + minStr + '_s' + maxStr print ( ' minStr:' , minStr , 'maxStr:' , maxStr , 'analysisName:' , analysisName ) df [ 'analysisname' ] = analysisName # should be filled in by self.ba.report #df['Condition'] = df['condition1'] #df['File Number'] = df['condition2'] #df['Sex'] = df['condition3'] #df['Region'] = df['condition4'] df [ 'filename' ] = [ os . path . splitext ( os . path . split ( x )[ 1 ])[ 0 ] for x in df [ 'file' ] . tolist ()] # print ( ' bExport.getReportDf() saving text file:' , textFilePath ) #df.to_csv(textFilePath, sep=',', index_label='index', mode='a') df . to_csv ( textFilePath , sep = ',' , index_label = 'index' , mode = 'w' ) return analysisName , df","title":"getReportDf()"},{"location":"bExport/#sanpy.bExport.bExport.report","text":"Get entire spikeDict as a Pandas DataFrame. Parameters: Name Type Description Default theMin float Start seconds of the analysis required theMax float Stop seconds of the analysis required Returns: Type Description df Pandas DataFrame Source code in sanpy/bExport.py def report ( self , theMin , theMax ): \"\"\" Get entire spikeDict as a Pandas DataFrame. Args: theMin (float): Start seconds of the analysis theMax (float): Stop seconds of the analysis Returns: df: Pandas DataFrame \"\"\" if theMin is None or theMax is None : return None df = pd . DataFrame ( self . ba . spikeDict ) df = df [ df [ 'thresholdSec' ] . between ( theMin , theMax , inclusive = True )] # added when trying to make scatterwidget for one file #print(' 20210426 adding columns in bExport.report()') #df['Condition'] = df['condition1'] #df['File Number'] = df['condition2'] #df['Sex'] = df['condition3'] #df['Region'] = df['condition4'] # make new column with sex/region encoded ''' tmpNewCol = 'RegSex' self.ba.masterDf[tmpNewCol] = '' for tmpRegion in ['Superior', 'Inferior']: for tmpSex in ['Male', 'Female']: newEncoding = tmpRegion[0] + tmpSex[0] regSex = self.ba.masterDf[ (self.ba.masterDf['Region']==tmpRegion) & (self.ba.masterDf['Sex']==tmpSex)] regSex = (self.ba.masterDf['Region']==tmpRegion) & (self.ba.masterDf['Sex']==tmpSex) print('newEncoding:', newEncoding, 'regSex:', regSex.shape) self.ba.masterDf.loc[regSex, tmpNewCol] = newEncoding ''' # want this but region/sex/condition are not defined print ( 'bExport.report()' ) print ( df . head ()) tmpNewCol = 'CellTypeSex' cellTypeStr = df [ 'cellType' ] . iloc [ 0 ] sexStr = df [ 'sex' ] . iloc [ 0 ] print ( 'cellTypeStr:' , cellTypeStr , 'sexStr:' , sexStr ) regSexEncoding = cellTypeStr + sexStr df [ tmpNewCol ] = regSexEncoding minStr = ' %.2f ' % ( theMin ) maxStr = ' %.2f ' % ( theMax ) minStr = minStr . replace ( '.' , '_' ) maxStr = maxStr . replace ( '.' , '_' ) tmpPath , tmpFile = os . path . split ( self . ba . file ) tmpFile , tmpExt = os . path . splitext ( tmpFile ) analysisName = tmpFile + '_s' + minStr + '_s' + maxStr print ( ' minStr:' , minStr , 'maxStr:' , maxStr , 'analysisName:' , analysisName ) df [ 'analysisname' ] = analysisName return df","title":"report()"},{"location":"bExport/#sanpy.bExport.bExport.report2","text":"Generate a report of spikes with spike times between theMin (sec) and theMax (sec). Parameters: Name Type Description Default theMin float Start seconds to save required theMax float Stop seconds to save required Returns: Type Description df Pandas DataFrame Source code in sanpy/bExport.py def report2 ( self , theMin , theMax ): \"\"\" Generate a report of spikes with spike times between theMin (sec) and theMax (sec). Args: theMin (float): Start seconds to save theMax (float): Stop seconds to save Returns: df: Pandas DataFrame \"\"\" newList = [] for spike in self . ba . spikeDict : # if current spike time is out of bounds then continue (e.g. it is not between theMin (sec) and theMax (sec) spikeTime_sec = self . ba . pnt2Sec_ ( spike [ 'thresholdPnt' ]) if spikeTime_sec < theMin or spikeTime_sec > theMax : continue spikeDict = OrderedDict () # use OrderedDict so Pandas output is in the correct order spikeDict [ 'Take Off Potential (s)' ] = self . ba . pnt2Sec_ ( spike [ 'thresholdPnt' ]) spikeDict [ 'Take Off Potential (ms)' ] = self . ba . pnt2Ms_ ( spike [ 'thresholdPnt' ]) spikeDict [ 'Take Off Potential (mV)' ] = spike [ 'thresholdVal' ] spikeDict [ 'AP Peak (ms)' ] = self . ba . pnt2Ms_ ( spike [ 'peakPnt' ]) spikeDict [ 'AP Peak (mV)' ] = spike [ 'peakVal' ] spikeDict [ 'AP Height (mV)' ] = spike [ 'peakHeight' ] spikeDict [ 'Pre AP Min (mV)' ] = spike [ 'preMinVal' ] #spikeDict['Post AP Min (mV)'] = spike['postMinVal'] # #spikeDict['AP Duration (ms)'] = spike['apDuration_ms'] spikeDict [ 'Early Diastolic Duration (ms)' ] = spike [ 'earlyDiastolicDuration_ms' ] spikeDict [ 'Early Diastolic Depolarization Rate (dV/s)' ] = spike [ 'earlyDiastolicDurationRate' ] # abb 202012 spikeDict [ 'Diastolic Duration (ms)' ] = spike [ 'diastolicDuration_ms' ] # spikeDict [ 'Inter-Spike-Interval (ms)' ] = spike [ 'isi_ms' ] spikeDict [ 'Spike Frequency (Hz)' ] = spike [ 'spikeFreq_hz' ] spikeDict [ 'Cycle Length (ms)' ] = spike [ 'cycleLength_ms' ] spikeDict [ 'Max AP Upstroke (dV/dt)' ] = spike [ 'preSpike_dvdt_max_val2' ] spikeDict [ 'Max AP Upstroke (mV)' ] = spike [ 'preSpike_dvdt_max_val' ] spikeDict [ 'Max AP Repolarization (dV/dt)' ] = spike [ 'postSpike_dvdt_min_val2' ] spikeDict [ 'Max AP Repolarization (mV)' ] = spike [ 'postSpike_dvdt_min_val' ] # half-width for widthDict in spike [ 'widths' ]: keyName = 'width_' + str ( widthDict [ 'halfHeight' ]) spikeDict [ keyName ] = widthDict [ 'widthMs' ] # errors #spikeDict['numError'] = spike['numError'] spikeDict [ 'errors' ] = spike [ 'errors' ] # append newList . append ( spikeDict ) df = pd . DataFrame ( newList ) return df","title":"report2()"},{"location":"bExport/#sanpy.bExport.bExport.saveReport","text":"Save a spike report for detected spikes between theMin (sec) and theMax (sec) Parameters: Name Type Description Default savefile str path to xlsx file required theMin float start/stop seconds of the analysis None theMax float start/stop seconds of the analysis None saveExcel bool xxx True alsoSaveTxt bool xxx True Returns: Type Description str analysisName df: df Source code in sanpy/bExport.py def saveReport ( self , savefile , theMin = None , theMax = None , saveExcel = True , alsoSaveTxt = True , verbose = True ): \"\"\" Save a spike report for detected spikes between theMin (sec) and theMax (sec) Args: savefile (str): path to xlsx file theMin (float): start/stop seconds of the analysis theMax (float): start/stop seconds of the analysis saveExcel (bool): xxx alsoSaveTxt (bool): xxx Return: str: analysisName df: df \"\"\" if theMin == None : theMin = 0 if theMax == None : theMax = self . ba . abf . sweepX [ - 1 ] # always grab a df to the entire analysis (not sure what I will do with this) #df = self.ba.report() # report() is my own 'bob' verbiage theRet = None if saveExcel and savefile : if verbose : print ( ' bExport.saveReport() saving user specified .xlsx file:' , savefile ) excelFilePath = savefile writer = pd . ExcelWriter ( excelFilePath , engine = 'xlsxwriter' ) # # cardiac style analysis to sheet 'cardiac' cardiac_df = self . report2 ( theMin , theMax ) # report2 is more 'cardiac' # # header sheet headerDict = OrderedDict () filePath , fileName = os . path . split ( self . ba . file ) headerDict [ 'File Name' ] = [ fileName ] headerDict [ 'File Path' ] = [ filePath ] headerDict [ 'Cell Type' ] = [ self . ba . detectionDict [ 'cellType' ]] headerDict [ 'Sex' ] = [ self . ba . detectionDict [ 'sex' ]] headerDict [ 'Condition' ] = [ self . ba . detectionDict [ 'condition' ]] # todo: get these params in ONE dict inside self.ba dateAnalyzed , timeAnalyzed = self . ba . dateAnalyzed . split ( ' ' ) headerDict [ 'Date Analyzed' ] = [ dateAnalyzed ] headerDict [ 'Time Analyzed' ] = [ timeAnalyzed ] headerDict [ 'Detection Type' ] = [ self . ba . detectionType ] headerDict [ 'dV/dt Threshold' ] = [ self . ba . detectionDict [ 'dvdtThreshold' ]] #headerDict['mV Threshold'] = [self.ba.mvThreshold] # abb 202012 headerDict [ 'Vm Threshold (mV)' ] = [ self . ba . detectionDict [ 'mvThreshold' ]] #headerDict['Median Filter (pnts)'] = [self.ba.medianFilter] headerDict [ 'Analysis Version' ] = [ sanpy . analysisVersion ] headerDict [ 'Interface Version' ] = [ sanpy . interfaceVersion ] #headerDict['Analysis Start (sec)'] = [self.ba.startSeconds] #headerDict['Analysis Stop (sec)'] = [self.ba.stopSeconds] headerDict [ 'Sweep Number' ] = [ self . ba . currentSweep ] headerDict [ 'Number of Sweeps' ] = [ self . ba . numSweeps ] headerDict [ 'Export Start (sec)' ] = [ float ( ' %.2f ' % ( theMin ))] # on export, x-axis of raw plot will be ouput headerDict [ 'Export Stop (sec)' ] = [ float ( ' %.2f ' % ( theMax ))] # on export, x-axis of raw plot will be ouput # 'stats' has xxx columns (name, mean, sd, se, n) headerDict [ 'stats' ] = [] for idx , col in enumerate ( cardiac_df ): headerDict [ col ] = [] # mean theMean = cardiac_df . mean () # skipna default is True theMean [ 'errors' ] = '' # sd theSD = cardiac_df . std () # skipna default is True theSD [ 'errors' ] = '' #se theSE = cardiac_df . sem () # skipna default is True theSE [ 'errors' ] = '' #n theN = cardiac_df . count () # skipna default is True theN [ 'errors' ] = '' statCols = [ 'mean' , 'sd' , 'se' , 'n' ] for j , stat in enumerate ( statCols ): if j == 0 : pass else : # need to append columns to keep Excel sheet columns in sync #for k,v in headerDict.items(): # headerDict[k].append('') headerDict [ 'File Name' ] . append ( '' ) headerDict [ 'File Path' ] . append ( '' ) headerDict [ 'Cell Type' ] . append ( '' ) headerDict [ 'Sex' ] . append ( '' ) headerDict [ 'Condition' ] . append ( '' ) # headerDict [ 'Date Analyzed' ] . append ( '' ) headerDict [ 'Time Analyzed' ] . append ( '' ) headerDict [ 'Detection Type' ] . append ( '' ) headerDict [ 'dV/dt Threshold' ] . append ( '' ) headerDict [ 'Vm Threshold (mV)' ] . append ( '' ) #headerDict['Median Filter (pnts)'].append('') headerDict [ 'Analysis Version' ] . append ( '' ) headerDict [ 'Interface Version' ] . append ( '' ) headerDict [ 'Sweep Number' ] . append ( '' ) headerDict [ 'Number of Sweeps' ] . append ( '' ) headerDict [ 'Export Start (sec)' ] . append ( '' ) headerDict [ 'Export Stop (sec)' ] . append ( '' ) # a dictionary key for each stat headerDict [ 'stats' ] . append ( stat ) for idx , col in enumerate ( cardiac_df ): #headerDict[col].append('') if stat == 'mean' : headerDict [ col ] . append ( theMean [ col ]) elif stat == 'sd' : headerDict [ col ] . append ( theSD [ col ]) elif stat == 'se' : headerDict [ col ] . append ( theSE [ col ]) elif stat == 'n' : headerDict [ col ] . append ( theN [ col ]) #print(headerDict) #for k,v in headerDict.items(): # print(k, v) # dict to pandas dataframe df = pd . DataFrame ( headerDict ) . T df . to_excel ( writer , sheet_name = 'summary' ) # set the column widths in excel sheet 'cardiac' columnWidth = 25 worksheet = writer . sheets [ 'summary' ] # pull worksheet object for idx , col in enumerate ( df ): # loop through all columns worksheet . set_column ( idx , idx , columnWidth ) # set column width # # 'params' sheet with all detection params # need to convert list values in dict to string (o.w. we get one row per item in list) exportDetectionDict = {} for k , v in self . ba . detectionDict . items (): if isinstance ( v , list ): v = f '\" { v } \"' exportDetectionDict [ k ] = v #print(' === \"params\" sheet exportDetectionDict:', exportDetectionDict) df = pd . DataFrame ( exportDetectionDict , index = [ 0 ]) . T # index=[0] needed when dict has all scalar values #print(' df:') #print(df) df . to_excel ( writer , sheet_name = 'params' ) columnWidth = 25 worksheet = writer . sheets [ 'params' ] # pull worksheet object worksheet . set_column ( 0 , 0 , columnWidth ) # set column width # # 'cardiac' sheet cardiac_df . to_excel ( writer , sheet_name = 'cardiac' ) # set the column widths in excel sheet 'cardiac' columnWidth = 20 worksheet = writer . sheets [ 'cardiac' ] # pull worksheet object for idx , col in enumerate ( cardiac_df ): # loop through all columns worksheet . set_column ( idx , idx , columnWidth ) # set column width # # entire (verbose) analysis to sheet 'bob' #df.to_excel(writer, sheet_name='bob') # # mean spike clip theseClips , theseClips_x , meanClip = self . ba . getSpikeClips ( theMin , theMax ) try : first_X = theseClips_x [ 0 ] #- theseClips_x[0][0] #if verbose: print(' bExport.saveReport() saving mean clip to sheet \"Avg Spike\" from', len(theseClips), 'clips') df = pd . DataFrame ( meanClip , first_X ) df . to_excel ( writer , sheet_name = 'Avg Spike' ) except ( IndexError ) as e : print ( 'warning: got bad spike clips in saveReport(). Usually happend when 1-2 spikes' ) #print('df:', df) writer . save () # # save a csv text file # analysisName = '' if alsoSaveTxt : # this also saves analysisName , df0 = self . getReportDf ( theMin , theMax , savefile ) # # save mean spike clip theseClips , theseClips_x , meanClip = self . ba . getSpikeClips ( theMin , theMax ) if len ( theseClips_x ) == 0 : pass else : first_X = theseClips_x [ 0 ] #- theseClips_x[0][0] first_X = np . array ( first_X ) first_X /= self . ba . abf . dataPointsPerMs # pnts to ms #if verbose: print(' bExport.saveReport() saving mean clip to sheet \"Avg Spike\" from', len(theseClips), 'clips') #dfClip = pd.DataFrame(meanClip, first_X) dfClip = pd . DataFrame . from_dict ({ 'xMs' : first_X , 'yVm' : meanClip }) # load clip based on analysisname (with start/stop seconds) analysisname = df0 [ 'analysisname' ] . iloc [ 0 ] # name with start/stop seconds print ( 'bExport.saveReport() analysisname:' , analysisname ) #print('analysisname:', analysisname) clipFileName = analysisname + '_clip.csv' tmpPath , tmpFile = os . path . split ( savefile ) tmpPath = os . path . join ( tmpPath , 'analysis' ) # dir is already created in getReportDf if not os . path . isdir ( tmpPath ): os . mkdir ( tmpPath ) clipSavePath = os . path . join ( tmpPath , clipFileName ) print ( ' clipSavePath:' , clipSavePath ) dfClip . to_csv ( clipSavePath ) # theRet = df0 # return analysisName , theRet","title":"saveReport()"},{"location":"desktop-application/","text":"The desktop application allows the user to load a folder of files (top table). Selecting a file will display both the derivative and raw membrane potential (middle two traces). Spike detection is then easily performed by specifying a threshold in either the derivative of the membrane potential or the membrane potential itself. Once spikes are detected, the detection parameters are overlaid over the raw membrane and derivative traces. Finally, there is an interface (lower table and colored plot) to inspect the detection parameters.","title":"Desktop Application"},{"location":"detectionParams/","text":"detectionParams module \u00a4 Root level functions to get and set detection parameters. getDefaultDetection () \u00a4 Get detection parameters including mapping from backend to human readable and long-format descriptions. Returns: Type Description dict The default detection dictionary. Source code in sanpy/detectionParams.py def getDefaultDetection (): \"\"\" Get detection parameters including mapping from backend to human readable and long-format descriptions. Returns: dict: The default detection dictionary. \"\"\" theDict = {} key = 'dvdtThreshold' theDict [ key ] = {} theDict [ key ][ 'defaultValue' ] = 100 theDict [ key ][ 'units' ] = 'dVdt' theDict [ key ][ 'humanName' ] = 'dV/dt Threshold' theDict [ key ][ 'errors' ] = ( '' ) theDict [ key ][ 'description' ] = 'dV/dt threshold for a spike, will be backed up to dvdt_percentOfMax and have xxx error when this fails' key = 'mvThreshold' theDict [ key ] = {} theDict [ key ][ 'defaultValue' ] = - 20 theDict [ key ][ 'units' ] = 'mV' theDict [ key ][ 'humanName' ] = 'mV Threshold' theDict [ key ][ 'errors' ] = ( '' ) theDict [ key ][ 'description' ] = 'mV threshold for spike AND minimum spike mV when detecting with dV/dt' key = 'dvdt_percentOfMax' theDict [ key ] = {} theDict [ key ][ 'defaultValue' ] = 0.1 theDict [ key ][ 'units' ] = 'Percent' theDict [ key ][ 'humanName' ] = 'dV/dt Percent of max' theDict [ key ][ 'errors' ] = ( '' ) theDict [ key ][ 'description' ] = 'For dV/dt detection, the final TOP is when dV/dt drops to this percent from dV/dt AP peak' key = 'onlyPeaksAbove_mV' theDict [ key ] = {} theDict [ key ][ 'defaultValue' ] = None theDict [ key ][ 'units' ] = 'mV' theDict [ key ][ 'humanName' ] = 'Accept Peaks Above (mV)' theDict [ key ][ 'errors' ] = ( '' ) theDict [ key ][ 'description' ] = 'For dV/dt detection, only accept APs above this value (mV)' key = 'doBackupSpikeVm' theDict [ key ] = {} theDict [ key ][ 'defaultValue' ] = True theDict [ key ][ 'units' ] = 'Boolean' theDict [ key ][ 'humanName' ] = 'Backup Vm Spikes' theDict [ key ][ 'errors' ] = ( '' ) theDict [ key ][ 'description' ] = 'If true, APs detected with just mV will be backed up until Vm falls to xxx' key = 'refractory_ms' theDict [ key ] = {} theDict [ key ][ 'defaultValue' ] = 170 theDict [ key ][ 'units' ] = 'ms' theDict [ key ][ 'humanName' ] = 'Minimum AP interval (ms)' theDict [ key ][ 'errors' ] = ( '' ) theDict [ key ][ 'description' ] = 'APs with interval (with respect to previous AP) less than this will be removed' key = 'peakWindow_ms' theDict [ key ] = {} theDict [ key ][ 'defaultValue' ] = 100 theDict [ key ][ 'units' ] = 'ms' theDict [ key ][ 'humanName' ] = 'Peak Window (ms)' theDict [ key ][ 'errors' ] = ( '' ) theDict [ key ][ 'description' ] = 'Window after TOP (ms) to seach for AP peak (mV)' key = 'dvdtPreWindow_ms' theDict [ key ] = {} theDict [ key ][ 'defaultValue' ] = 10 theDict [ key ][ 'units' ] = 'ms' theDict [ key ][ 'humanName' ] = 'dV/dt Pre Window (ms)' theDict [ key ][ 'errors' ] = ( '' ) theDict [ key ][ 'description' ] = 'Window (ms) to search before each TOP for real threshold crossing' key = 'mdp_ms' theDict [ key ] = {} theDict [ key ][ 'defaultValue' ] = 250 theDict [ key ][ 'units' ] = 'ms' theDict [ key ][ 'humanName' ] = 'Pre AP MDP window (ms)' theDict [ key ][ 'errors' ] = ( '' ) theDict [ key ][ 'description' ] = 'Window (ms) before an AP to look for MDP' key = 'avgWindow_ms' theDict [ key ] = {} theDict [ key ][ 'defaultValue' ] = 5 theDict [ key ][ 'units' ] = 'ms' theDict [ key ][ 'humanName' ] = '' theDict [ key ][ 'errors' ] = ( '' ) theDict [ key ][ 'description' ] = 'Window (ms) to calculate MDP (mV) as a mean rather than mV at single point for MDP' key = 'halfHeights' theDict [ key ] = {} theDict [ key ][ 'defaultValue' ] = [ 10 , 20 , 50 , 80 , 90 ] theDict [ key ][ 'units' ] = '' theDict [ key ][ 'humanName' ] = 'AP Durations (%)' theDict [ key ][ 'errors' ] = ( '' ) theDict [ key ][ 'description' ] = 'AP Durations as percent of AP height (AP Peak (mV) - TOP (mV))' key = 'halfWidthWindow_ms' theDict [ key ] = {} theDict [ key ][ 'defaultValue' ] = 200 theDict [ key ][ 'units' ] = 'ms' theDict [ key ][ 'humanName' ] = 'Half Width Window (ms)' theDict [ key ][ 'errors' ] = ( '' ) theDict [ key ][ 'description' ] = 'Window (ms) after TOP to look for AP Durations' key = 'medianFilter' theDict [ key ] = {} theDict [ key ][ 'defaultValue' ] = 0 theDict [ key ][ 'units' ] = 'points' theDict [ key ][ 'humanName' ] = 'Median Filter Points' theDict [ key ][ 'errors' ] = ( '' ) theDict [ key ][ 'description' ] = 'Number of points in median filter, must be odd, 0 for no filter' key = 'SavitzkyGolay_pnts' theDict [ key ] = {} theDict [ key ][ 'defaultValue' ] = 5 theDict [ key ][ 'units' ] = 'points' theDict [ key ][ 'humanName' ] = 'SavitzkyGolay Filter Points' theDict [ key ][ 'errors' ] = ( '' ) theDict [ key ][ 'description' ] = 'Number of points in SavitzkyGolay filter, must be odd, 0 for no filter' key = 'SavitzkyGolay_poly' theDict [ key ] = {} theDict [ key ][ 'defaultValue' ] = 2 theDict [ key ][ 'units' ] = '' theDict [ key ][ 'humanName' ] = 'Savitzky-Golay Filter Polynomial Degree' theDict [ key ][ 'errors' ] = ( '' ) theDict [ key ][ 'description' ] = 'The degree of the polynomial for Savitzky-Golay filter' key = 'spikeClipWidth_ms' theDict [ key ] = {} theDict [ key ][ 'defaultValue' ] = 500 theDict [ key ][ 'units' ] = 'ms' theDict [ key ][ 'humanName' ] = 'AP Clip Width (ms)' theDict [ key ][ 'errors' ] = ( '' ) theDict [ key ][ 'description' ] = 'The width/duration of generated AP clips' return theDict . copy ()","title":"detectionParams"},{"location":"detectionParams/#detectionparams-module","text":"Root level functions to get and set detection parameters.","title":"detectionParams module"},{"location":"detectionParams/#sanpy.detectionParams.getDefaultDetection","text":"Get detection parameters including mapping from backend to human readable and long-format descriptions. Returns: Type Description dict The default detection dictionary. Source code in sanpy/detectionParams.py def getDefaultDetection (): \"\"\" Get detection parameters including mapping from backend to human readable and long-format descriptions. Returns: dict: The default detection dictionary. \"\"\" theDict = {} key = 'dvdtThreshold' theDict [ key ] = {} theDict [ key ][ 'defaultValue' ] = 100 theDict [ key ][ 'units' ] = 'dVdt' theDict [ key ][ 'humanName' ] = 'dV/dt Threshold' theDict [ key ][ 'errors' ] = ( '' ) theDict [ key ][ 'description' ] = 'dV/dt threshold for a spike, will be backed up to dvdt_percentOfMax and have xxx error when this fails' key = 'mvThreshold' theDict [ key ] = {} theDict [ key ][ 'defaultValue' ] = - 20 theDict [ key ][ 'units' ] = 'mV' theDict [ key ][ 'humanName' ] = 'mV Threshold' theDict [ key ][ 'errors' ] = ( '' ) theDict [ key ][ 'description' ] = 'mV threshold for spike AND minimum spike mV when detecting with dV/dt' key = 'dvdt_percentOfMax' theDict [ key ] = {} theDict [ key ][ 'defaultValue' ] = 0.1 theDict [ key ][ 'units' ] = 'Percent' theDict [ key ][ 'humanName' ] = 'dV/dt Percent of max' theDict [ key ][ 'errors' ] = ( '' ) theDict [ key ][ 'description' ] = 'For dV/dt detection, the final TOP is when dV/dt drops to this percent from dV/dt AP peak' key = 'onlyPeaksAbove_mV' theDict [ key ] = {} theDict [ key ][ 'defaultValue' ] = None theDict [ key ][ 'units' ] = 'mV' theDict [ key ][ 'humanName' ] = 'Accept Peaks Above (mV)' theDict [ key ][ 'errors' ] = ( '' ) theDict [ key ][ 'description' ] = 'For dV/dt detection, only accept APs above this value (mV)' key = 'doBackupSpikeVm' theDict [ key ] = {} theDict [ key ][ 'defaultValue' ] = True theDict [ key ][ 'units' ] = 'Boolean' theDict [ key ][ 'humanName' ] = 'Backup Vm Spikes' theDict [ key ][ 'errors' ] = ( '' ) theDict [ key ][ 'description' ] = 'If true, APs detected with just mV will be backed up until Vm falls to xxx' key = 'refractory_ms' theDict [ key ] = {} theDict [ key ][ 'defaultValue' ] = 170 theDict [ key ][ 'units' ] = 'ms' theDict [ key ][ 'humanName' ] = 'Minimum AP interval (ms)' theDict [ key ][ 'errors' ] = ( '' ) theDict [ key ][ 'description' ] = 'APs with interval (with respect to previous AP) less than this will be removed' key = 'peakWindow_ms' theDict [ key ] = {} theDict [ key ][ 'defaultValue' ] = 100 theDict [ key ][ 'units' ] = 'ms' theDict [ key ][ 'humanName' ] = 'Peak Window (ms)' theDict [ key ][ 'errors' ] = ( '' ) theDict [ key ][ 'description' ] = 'Window after TOP (ms) to seach for AP peak (mV)' key = 'dvdtPreWindow_ms' theDict [ key ] = {} theDict [ key ][ 'defaultValue' ] = 10 theDict [ key ][ 'units' ] = 'ms' theDict [ key ][ 'humanName' ] = 'dV/dt Pre Window (ms)' theDict [ key ][ 'errors' ] = ( '' ) theDict [ key ][ 'description' ] = 'Window (ms) to search before each TOP for real threshold crossing' key = 'mdp_ms' theDict [ key ] = {} theDict [ key ][ 'defaultValue' ] = 250 theDict [ key ][ 'units' ] = 'ms' theDict [ key ][ 'humanName' ] = 'Pre AP MDP window (ms)' theDict [ key ][ 'errors' ] = ( '' ) theDict [ key ][ 'description' ] = 'Window (ms) before an AP to look for MDP' key = 'avgWindow_ms' theDict [ key ] = {} theDict [ key ][ 'defaultValue' ] = 5 theDict [ key ][ 'units' ] = 'ms' theDict [ key ][ 'humanName' ] = '' theDict [ key ][ 'errors' ] = ( '' ) theDict [ key ][ 'description' ] = 'Window (ms) to calculate MDP (mV) as a mean rather than mV at single point for MDP' key = 'halfHeights' theDict [ key ] = {} theDict [ key ][ 'defaultValue' ] = [ 10 , 20 , 50 , 80 , 90 ] theDict [ key ][ 'units' ] = '' theDict [ key ][ 'humanName' ] = 'AP Durations (%)' theDict [ key ][ 'errors' ] = ( '' ) theDict [ key ][ 'description' ] = 'AP Durations as percent of AP height (AP Peak (mV) - TOP (mV))' key = 'halfWidthWindow_ms' theDict [ key ] = {} theDict [ key ][ 'defaultValue' ] = 200 theDict [ key ][ 'units' ] = 'ms' theDict [ key ][ 'humanName' ] = 'Half Width Window (ms)' theDict [ key ][ 'errors' ] = ( '' ) theDict [ key ][ 'description' ] = 'Window (ms) after TOP to look for AP Durations' key = 'medianFilter' theDict [ key ] = {} theDict [ key ][ 'defaultValue' ] = 0 theDict [ key ][ 'units' ] = 'points' theDict [ key ][ 'humanName' ] = 'Median Filter Points' theDict [ key ][ 'errors' ] = ( '' ) theDict [ key ][ 'description' ] = 'Number of points in median filter, must be odd, 0 for no filter' key = 'SavitzkyGolay_pnts' theDict [ key ] = {} theDict [ key ][ 'defaultValue' ] = 5 theDict [ key ][ 'units' ] = 'points' theDict [ key ][ 'humanName' ] = 'SavitzkyGolay Filter Points' theDict [ key ][ 'errors' ] = ( '' ) theDict [ key ][ 'description' ] = 'Number of points in SavitzkyGolay filter, must be odd, 0 for no filter' key = 'SavitzkyGolay_poly' theDict [ key ] = {} theDict [ key ][ 'defaultValue' ] = 2 theDict [ key ][ 'units' ] = '' theDict [ key ][ 'humanName' ] = 'Savitzky-Golay Filter Polynomial Degree' theDict [ key ][ 'errors' ] = ( '' ) theDict [ key ][ 'description' ] = 'The degree of the polynomial for Savitzky-Golay filter' key = 'spikeClipWidth_ms' theDict [ key ] = {} theDict [ key ][ 'defaultValue' ] = 500 theDict [ key ][ 'units' ] = 'ms' theDict [ key ][ 'humanName' ] = 'AP Clip Width (ms)' theDict [ key ][ 'errors' ] = ( '' ) theDict [ key ][ 'description' ] = 'The width/duration of generated AP clips' return theDict . copy ()","title":"getDefaultDetection()"},{"location":"install/","text":"SanPy will run on macOS, Microsoft Windows, or Linux. Assuming you have the following Python 3.7.x pip git (optional) Install the desktop application \u00a4 Option 1) Install using ./install \u00a4 # If you have git installed. # Clone the github repository (this will create a SanPy/ folder). git clone https://github.com/cudmore/SanPy.git # If you do not have git installed you can download the .zip file manually. # In a browser, go to 'https://github.com/cudmore/SanPy'. # Click green button 'Clone or download'. # Select 'Download ZIP'. # Once downloaded, manually extract the contents of the .zip file and continue following this tutorial. # Change into the cloned or downloaded 'SanPy/' folder. cd SanPy # Install ./install # Run ./run Option 2) Install manually \u00a4 # clone the github repository (this will create a SanPy/ folder) git clone https://github.com/cudmore/SanPy.git # change into the cloned SanPy folder cd SanPy # create a Python3 virtual environment in 'sanpy_env/' folder python -m venv sanpy_env # [OR] if python is bound to Python 2 (check with 'python --version') python3 -m venv sanpy_env # activate the virtual environment in sanpy_env/ source sanpy_env/bin/activate # install the package pip install . # [OR] install the required python packages (into the activated virtual environment) # pip install -r requirements.txt Running the desktop application \u00a4 Option 1) Using ./run \u00a4 cd SanPy ./run Option 2) Manually \u00a4 # activate the virtual environment in sanpy_env/ cd SanPy source sanpy_env/bin/activate # run the desktop application python sanpy/sanpy_app.py","title":"Install"},{"location":"install/#install-the-desktop-application","text":"","title":"Install the desktop application"},{"location":"install/#option-1-install-using-install","text":"# If you have git installed. # Clone the github repository (this will create a SanPy/ folder). git clone https://github.com/cudmore/SanPy.git # If you do not have git installed you can download the .zip file manually. # In a browser, go to 'https://github.com/cudmore/SanPy'. # Click green button 'Clone or download'. # Select 'Download ZIP'. # Once downloaded, manually extract the contents of the .zip file and continue following this tutorial. # Change into the cloned or downloaded 'SanPy/' folder. cd SanPy # Install ./install # Run ./run","title":"Option 1) Install using ./install"},{"location":"install/#option-2-install-manually","text":"# clone the github repository (this will create a SanPy/ folder) git clone https://github.com/cudmore/SanPy.git # change into the cloned SanPy folder cd SanPy # create a Python3 virtual environment in 'sanpy_env/' folder python -m venv sanpy_env # [OR] if python is bound to Python 2 (check with 'python --version') python3 -m venv sanpy_env # activate the virtual environment in sanpy_env/ source sanpy_env/bin/activate # install the package pip install . # [OR] install the required python packages (into the activated virtual environment) # pip install -r requirements.txt","title":"Option 2) Install manually"},{"location":"install/#running-the-desktop-application","text":"","title":"Running the desktop application"},{"location":"install/#option-1-using-run","text":"cd SanPy ./run","title":"Option 1) Using ./run"},{"location":"install/#option-2-manually","text":"# activate the virtual environment in sanpy_env/ cd SanPy source sanpy_env/bin/activate # run the desktop application python sanpy/sanpy_app.py","title":"Option 2) Manually"},{"location":"methods/","text":"What spike parameters are detected? \u00a4 We are following the cardiac myocyte nomenclature from this paper: Larson, et al (2013) Depressed pacemaker activity of sinoatrial node myocytes contributes to the age-dependent decline in maximum heart rate. PNAS 110(44):18011-18016 MDP and Vmax were defined as the most negative and positive membrane potentials, respectively Take-off potential (TOP) was defined as the membrane potential when the first derivative of voltage with respect to time (dV/dt) reached 10% of its maximum value Cycle length was defined as the interval between MDPs in successive APs The maximum rates of the AP upstroke and repolarization were taken as the maximum and minimum values of the first derivative (dV/dtmax and dV/dtmin, respectively) [[[REMOVED 20210501]]] Action potential duration (APD) was defined as the interval between the TOP and the subsequent MDP APD_50 and APD_90 were defined as the interval between the TOP and 50% and 90% repolarization, respectively The diastolic duration was defined as the interval between MDP and TOP The early diastolic depolarization rate was estimated as the slope of a linear fit between 10% and 50% of the diastolic duration and the early diastolic duration was the corresponding time interval The nonlinear late diastolic depolarization phase was estimated as the duration between 1% and 10% dV/dt Detection Errors \u00a4 When we encounter errors during spike detection, they are stored for each spike in ['errors']. Each error has a name like 'dvdtPercent' as follows dvdtPercent: Error searching for percent (10%) of dvdt max. When this occurs, the TOP (mV) of a spike will be more depolarized than it should be. preMin: Error searching for spike pre min, the MDP before a spike. This can occur on the first spike if it is close to the beginning of the recording. postMin: fitEDD: Error while fitting slope of EDD. preSpikeDvDt: Error while searching for peak in max ap upstroke (dV/dt) between spike threshold (TOP) and the peak in the first derivative of Vm (dV/dt). cycleLength: Usually occurs on last spike when looking for next MDP. spikeWidth: Error finding a particular spike with (AP_Dur). Usually occurs when spikes are too broad, can increase detection parameter hwWindow_ms .","title":"Methods"},{"location":"methods/#what-spike-parameters-are-detected","text":"We are following the cardiac myocyte nomenclature from this paper: Larson, et al (2013) Depressed pacemaker activity of sinoatrial node myocytes contributes to the age-dependent decline in maximum heart rate. PNAS 110(44):18011-18016 MDP and Vmax were defined as the most negative and positive membrane potentials, respectively Take-off potential (TOP) was defined as the membrane potential when the first derivative of voltage with respect to time (dV/dt) reached 10% of its maximum value Cycle length was defined as the interval between MDPs in successive APs The maximum rates of the AP upstroke and repolarization were taken as the maximum and minimum values of the first derivative (dV/dtmax and dV/dtmin, respectively) [[[REMOVED 20210501]]] Action potential duration (APD) was defined as the interval between the TOP and the subsequent MDP APD_50 and APD_90 were defined as the interval between the TOP and 50% and 90% repolarization, respectively The diastolic duration was defined as the interval between MDP and TOP The early diastolic depolarization rate was estimated as the slope of a linear fit between 10% and 50% of the diastolic duration and the early diastolic duration was the corresponding time interval The nonlinear late diastolic depolarization phase was estimated as the duration between 1% and 10% dV/dt","title":"What spike parameters are detected?"},{"location":"methods/#detection-errors","text":"When we encounter errors during spike detection, they are stored for each spike in ['errors']. Each error has a name like 'dvdtPercent' as follows dvdtPercent: Error searching for percent (10%) of dvdt max. When this occurs, the TOP (mV) of a spike will be more depolarized than it should be. preMin: Error searching for spike pre min, the MDP before a spike. This can occur on the first spike if it is close to the beginning of the recording. postMin: fitEDD: Error while fitting slope of EDD. preSpikeDvDt: Error while searching for peak in max ap upstroke (dV/dt) between spike threshold (TOP) and the peak in the first derivative of Vm (dV/dt). cycleLength: Usually occurs on last spike when looking for next MDP. spikeWidth: Error finding a particular spike with (AP_Dur). Usually occurs when spikes are too broad, can increase detection parameter hwWindow_ms .","title":"Detection Errors"},{"location":"open-source/","text":"Technologies used \u00a4 Backend \u00a4 Python Pandas NumPy pyABF - Package to open Axon Binary Format (ABF) files heka_reader - Python file to read Heka Data files XlsxWriter - Desktop Application \u00a4 PyQt - Desktop application interface PyQtGraph - Derived from PyQt and used to make fast plots Matplotlib - Desktop application plotting Web application \u00a4 Plotly Python Plotly Dash - Web application interface Dash Bootstrap components Other software \u00a4 ParamAP - Standardized parameterization of sinoatrial node myocyte action potentials stimfit - A program for viewing and analyzing electrophysiological data C++ libraries biosig - A C/C++ library providing reading and writing routines for biosignal data formats sigviewer - SigViewer is a viewing application for biosignals.","title":"Open Source"},{"location":"open-source/#technologies-used","text":"","title":"Technologies used"},{"location":"open-source/#backend","text":"Python Pandas NumPy pyABF - Package to open Axon Binary Format (ABF) files heka_reader - Python file to read Heka Data files XlsxWriter -","title":"Backend"},{"location":"open-source/#desktop-application","text":"PyQt - Desktop application interface PyQtGraph - Derived from PyQt and used to make fast plots Matplotlib - Desktop application plotting","title":"Desktop Application"},{"location":"open-source/#web-application","text":"Plotly Python Plotly Dash - Web application interface Dash Bootstrap components","title":"Web application"},{"location":"open-source/#other-software","text":"ParamAP - Standardized parameterization of sinoatrial node myocyte action potentials stimfit - A program for viewing and analyzing electrophysiological data C++ libraries biosig - A C/C++ library providing reading and writing routines for biosignal data formats sigviewer - SigViewer is a viewing application for biosignals.","title":"Other software"},{"location":"web-application/","text":"The browser based web application provides the same interface for analysis as the desktop application. Once data is analyzed, Pooling allows browsing detection parameters across any number of files. Install the web application \u00a4 Please note, this is experimental and does not have all functions implemented. Please use the desktop version instead. cd SanPy/dash pip install -r requirements.txt Running the web applications \u00a4 Run the web application to analyze raw data cd SanPy/dash python app2.py The web application for analysis is available at http://localhost:8000 Run the web application to browse and pool saved analysis cd SanPy/dash python bBrowser_app.py The web application for browsing and pooling saved analysis is available at http://localhost:8050","title":"Web Application"},{"location":"web-application/#install-the-web-application","text":"Please note, this is experimental and does not have all functions implemented. Please use the desktop version instead. cd SanPy/dash pip install -r requirements.txt","title":"Install the web application"},{"location":"web-application/#running-the-web-applications","text":"Run the web application to analyze raw data cd SanPy/dash python app2.py The web application for analysis is available at http://localhost:8000 Run the web application to browse and pool saved analysis cd SanPy/dash python bBrowser_app.py The web application for browsing and pooling saved analysis is available at http://localhost:8050","title":"Running the web applications"}]}